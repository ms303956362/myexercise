{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IERG 6130 Assignment 2: Value Function Approximation in RL\n",
    "\n",
    "*2019-2020 2nd term, IERG 6130: Reinforcement Learning and Beyond. Department of Information Engineering, The Chinese University of Hong Kong. Course Instructor: Professor ZHOU Bolei. Assignment author: PENG Zhenghao.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Student Name | Student ID |\n",
    "| :----: | :----: |\n",
    "| TYPE_YOUR_NAME_HERE | TYPE_YOUR_STUDENT_ID_HERE |\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the assignment 2 of our RL course. \n",
    "\n",
    "You need to go through this self-contained notebook, which contains many TODOs in part of the cells and has special `[TODO]` signs. You need to finish all TODOs. Some of them may be easy such as uncommenting a line, some of them may be difficult such as implementing a function. You can find them by searching the `[TODO]` symbol. However, we suggest you to go through the notebook step by step, which would give you a better understanding of the content.\n",
    "\n",
    "You are encouraged to add more code on extra cells at the end of the each section to investigate the problems you think interesting. At the end of the file, we left a place for you to optionally write comments (Yes, please give us rewards so we can keep improving the assignment!). \n",
    "\n",
    "Please report any code bugs to us via cuhkrlcourse@googlegroups.com or via github issue.\n",
    "\n",
    "Before you get start, remember to follow the instruction at https://github.com/cuhkrlcourse/ierg6130 to setup your environment.\n",
    "\n",
    "We will cover the following knowledge in this assignment:\n",
    "\n",
    "1. The n-step Sarsa and Q learning algorithm\n",
    "2. Linear function as the approximation of value function\n",
    "3. Feature construction\n",
    "4. Neural network based function approximation\n",
    "5. Naive implementation of Deep Q Network using pytorch\n",
    "\n",
    "As shown in the following figure, in the first section of notebook, we build a basic RL pipeline. In the second section, we implement the linear function as approximator and also introduces feature construction technique. In the third section, we implement a simple neural network simply using Numpy package. \n",
    "\n",
    "![](overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Section 1: Basic Reinforcement Learning Pipeline\n",
    "\n",
    "(5 / 100 points)\n",
    "\n",
    "In this section, we will prepare several functions for evaulation and training of RL algorithms. We will also build an `AbstractTrainer` class used as a general framework which left blanks for different function approximation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import *\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def evaluate(policy, num_episodes=1, seed=0, env_name='FrozenLake8x8-v0',\n",
    "             render=False):\n",
    "    \"\"\"This function evaluate the given policy and return the mean episode \n",
    "    reward.\n",
    "    :param policy: a function whose input is the observation\n",
    "    :param num_episodes: number of episodes you wish to run\n",
    "    :param seed: the random seed\n",
    "    :param env_name: the name of the environment\n",
    "    :param render: a boolean flag indicating whether to render policy\n",
    "    :return: the averaged episode reward of the given policy.\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    env.seed(seed)\n",
    "    rewards = []\n",
    "    if render: num_episodes = 1\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done, info = env.step(act)\n",
    "            act = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "                wait(sleep=0.05)\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "    if render:\n",
    "        env.close()\n",
    "    return np.mean(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    \"\"\"Run the trainer and report progress, agnostic to the class of trainer\n",
    "    :param trainer_cls: A trainer class \n",
    "    :param config: A dict\n",
    "    :param reward_threshold: the reward threshold to break the training\n",
    "    :return: The trained trainer and a dataframe containing learning progress\n",
    "    \"\"\"\n",
    "    assert inspect.isclass(trainer_cls)\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    trainer = trainer_cls(config)\n",
    "    config = trainer.config\n",
    "    start = now = time.time()\n",
    "    stats = []\n",
    "    for i in range(config['max_iteration'] + 1):\n",
    "        stat = trainer.train()\n",
    "        stats.append(stat or {})\n",
    "        if i % config['evaluate_interval'] == 0 or \\\n",
    "                i == config[\"max_iteration\"]:\n",
    "            reward = trainer.evaluate(config.get(\"evaluate_num_episodes\", 50))\n",
    "            print(\"({:.1f}s,+{:.1f}s)\\tIteration {}, current mean episode \"\n",
    "                  \"reward is {}. {}\".format(\n",
    "                time.time() - start, time.time() - now, i, reward,\n",
    "                {k: round(np.mean(v), 4) for k, v in\n",
    "                 stat.items()} if stat else \"\"))\n",
    "            now = time.time()\n",
    "        if reward_threshold is not None and reward > reward_threshold:\n",
    "            print(\"In {} iteration, current mean episode reward {:.3f} is \"\n",
    "                  \"greater than reward threshold {}. Congratulation! Now we \"\n",
    "                  \"exit the training process.\".format(\n",
    "                i, reward, reward_threshold))\n",
    "            break\n",
    "    return trainer, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve TODOs and remove \"pass\"\n",
    "\n",
    "default_config = dict(\n",
    "    env_name=\"CartPole-v0\",\n",
    "    max_iteration=1000,\n",
    "    max_episode_length=1000,\n",
    "    evaluate_interval=100,\n",
    "    gamma=0.99,\n",
    "    eps=0.3,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "\n",
    "class AbstractTrainer:\n",
    "    \"\"\"This is the abstract class for value-based RL trainer. We will inherent\n",
    "    the specify algorithm's trainer from this abstract class, so that we can\n",
    "    reuse the codes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = merge_config(config, default_config)\n",
    "\n",
    "        # Create the environment\n",
    "        self.env_name = self.config['env_name']\n",
    "        self.env = gym.make(self.env_name)\n",
    "        if self.env_name == \"Pong-ram-v0\":\n",
    "            self.env = wrap_deepmind_ram(self.env)\n",
    "\n",
    "        # Apply the random seed\n",
    "        self.seed = self.config[\"seed\"]\n",
    "        np.random.seed(self.seed)\n",
    "        self.env.seed(self.seed)\n",
    "\n",
    "        # We set self.obs_dim to the number of possible observation\n",
    "        # if observation space is discrete, otherwise the number\n",
    "        # of observation's dimensions. The same to self.act_dim.\n",
    "        if isinstance(self.env.observation_space, gym.spaces.box.Box):\n",
    "            assert len(self.env.observation_space.shape) == 1\n",
    "            self.obs_dim = self.env.observation_space.shape[0]\n",
    "            self.discrete_obs = False\n",
    "        elif isinstance(self.env.observation_space,\n",
    "                        gym.spaces.discrete.Discrete):\n",
    "            self.obs_dim = self.env.observation_space.n\n",
    "            self.discrete_obs = True\n",
    "        else:\n",
    "            raise ValueError(\"Wrong observation space!\")\n",
    "\n",
    "        if isinstance(self.env.action_space, gym.spaces.box.Box):\n",
    "            assert len(self.env.action_space.shape) == 1\n",
    "            self.act_dim = self.env.action_space.shape[0]\n",
    "        elif isinstance(self.env.action_space, gym.spaces.discrete.Discrete):\n",
    "            self.act_dim = self.env.action_space.n\n",
    "        else:\n",
    "            raise ValueError(\"Wrong action space!\")\n",
    "\n",
    "        self.eps = self.config['eps']\n",
    "\n",
    "        # You need to setup the parameter for your function approximator.\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        self.parameters = None\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the \"\n",
    "            \"Trainer._initialize_parameters() function.\")\n",
    "\n",
    "    def process_state(self, state):\n",
    "        \"\"\"Preprocess the state (observation) if necessary\"\"\"\n",
    "        processed_state = state\n",
    "        return processed_state\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        \"\"\"Approximate the state value of given state.\n",
    "        This is a private function.\n",
    "        Note that you should NOT preprocess the state here.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.compute_values() function.\")\n",
    "\n",
    "    def compute_action(self, processed_state, eps=None):\n",
    "        \"\"\"Compute the action given the state. Note that the input\n",
    "        is the processed state.\"\"\"\n",
    "\n",
    "        values = self.compute_values(processed_state)\n",
    "        assert values.ndim == 1, values.shape\n",
    "\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "\n",
    "        # [TODO] Implement the epsilon-greedy policy here. We have `eps`\n",
    "        #  probability to choose a uniformly random action in action_space,\n",
    "        #  otherwise choose action that maximizes the values.\n",
    "        # Hint: Use the function of self.env.action_space to sample random\n",
    "        # action.\n",
    "        if np.random.rand() < eps:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(values)\n",
    "\n",
    "    def evaluate(self, num_episodes=50, *args, **kwargs):\n",
    "        \"\"\"Use the function you write to evaluate current policy.\n",
    "        Return the mean episode reward of 50 episodes.\"\"\"\n",
    "        policy = lambda raw_state: self.compute_action(\n",
    "            self.process_state(raw_state), eps=0.0)\n",
    "        result = evaluate(policy, num_episodes, seed=self.seed,\n",
    "                          env_name=self.env_name, *args, **kwargs)\n",
    "        return result\n",
    "\n",
    "    def compute_gradient(self, *args, **kwargs):\n",
    "        \"\"\"Compute the gradient.\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the Trainer.compute_gradient() function.\")\n",
    "\n",
    "    def apply_gradient(self, *args, **kwargs):\n",
    "        \"\"\"Compute the gradient\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the Trainer.apply_gradient() function.\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.train() function.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Average episode reward for a random policy in 500 episodes in CartPole-v0:  22.068\n"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "class TestTrainer(AbstractTrainer):\n",
    "    \"\"\"This class is used for testing. We don't really train anything.\"\"\"\n",
    "    def compute_values(self, state):\n",
    "        return np.random.random_sample(size=self.act_dim)\n",
    "    def initialize_parameters(self):\n",
    "        self.parameters = np.random.random_sample(size=(self.obs_dim, self.act_dim))\n",
    "    \n",
    "t = TestTrainer(dict(env_name=\"CartPole-v0\"))\n",
    "obs = t.env.observation_space.sample()\n",
    "processed = t.process_state(obs)\n",
    "assert processed.shape == (4, )\n",
    "assert np.all(processed == obs)\n",
    "# Test compute_action\n",
    "values = t.compute_values(processed)\n",
    "correct_act = np.argmax(values)\n",
    "assert t.compute_action(processed, eps=0) == correct_act\n",
    "print(\"Average episode reward for a random policy in 500 episodes in CartPole-v0: \",\n",
    "      t.evaluate(num_episodes=500))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Linear function approximation\n",
    "\n",
    "In this section, we implement a simple linear function whose input is the state (or the processed state) and output is the state-action values.\n",
    "\n",
    "First, we implement a `LinearTrainer` class which implements (1). Linear function approximation and (2). n-step semi-gradient method to update the linear function.\n",
    "\n",
    "Then we further implement a `LinearTrainerWithFeatureConstruction` class which processes the input state and provide polynomial features which increase the utility of linear function approximation.\n",
    "\n",
    "We refer the Chapter 9.4 (linear method), 9.5 (feature construction), and 10.2 (n-step semi-gradient method) of the RL textbook to you.\n",
    "\n",
    "In this section, we leverage the n-step semi-gradient Sarsa as the training algorithm. What is the \"correct value\" of an action $a_t$ at state $s_t$ in one-step case? We consider it is $r_t + \\gamma Q(s_{t+1}, a_{t+1})$ and thus lead to the TD error $TD = r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)$. In n-step case, the target value of Q is natually extended:\n",
    "\n",
    "$$Q(s_t, a_t) = \\sum_{i=t}^{t+n-1}\\gamma^{i-t}r_i + \\gamma^n Q(s_{t+n}, a_{t+n})$$\n",
    "\n",
    "We follow the pipeline depicted in Chapter 10.2 (page 247) of the textbook to implement this logic. Note that notation of the time step of reward is different in this assignment and in the textbook. In textbook, the reward $R_{t+1}$ is the reward when apply action $a_{t}$ to the environment at state $s_t$. In the equation above the $r_t$ has exactly the same meaning. In the code below, we store the states, actions and rewards to a list during training. You need to make sure the indices of these list, like the `tau` in  `actions[tau]` has the correct meaning.\n",
    "\n",
    "After computing the target Q value, we need to derive the gradient to update the parameters. Consider a loss function, the Mean Square Error between the target Q value and the output Q value: \n",
    "\n",
    "$$\\text{loss} = \\cfrac{1}{2}[\\sum_{i=t}^{t+n-1}\\gamma^{i-t}r_i + \\gamma^n Q(s_{t+n}, a_{t+n}) - Q(s_t, a_t)]^2$$\n",
    "\n",
    "Compute the gradient of Loss with respect to the Q function:\n",
    "\n",
    "$$\\cfrac{d \\text{loss}}{d Q} = -(\\sum_{i=t}^{t+n-1}\\gamma^{i-t}r_i + \\gamma^n Q(s_{t+n}, a_{t+n}) - Q(s_t, a_t))$$\n",
    "\n",
    "According to the chain rule, the gradient of the loss w.r.t. the parameter ($W$) is:\n",
    "\n",
    "$$\\cfrac{d \\text{loss}}{d W} = -(\\sum_{i=t}^{t+n-1}\\gamma^{i-t}r_i + \\gamma^n Q(s_{t+n}, a_{t+n}) - Q(s_t, a_t))\\cfrac{d Q}{d W}$$\n",
    "\n",
    "To minimize the loss, we only need to descent the gradient:\n",
    "\n",
    "$$W = W - lr \\cfrac{d \\text{loss}}{d W}$$\n",
    "\n",
    "wherein $lr$ is the learning rate. Therefore, in conclusion the update rule of parameters is:\n",
    "\n",
    "$$W = W + lr (\\sum_{i=t}^{t+n-1}\\gamma^{i-t}r_i + \\gamma^n Q(s_{t+n}, a_{t+n}) - Q(s_t, a_t))\\cfrac{d Q}{d W}$$\n",
    "\n",
    "In the following codes, we denote $G = \\sum_{i=t}^{t+n-1}\\gamma^{i-t}r_i + \\gamma^n Q(s_{t+n}, a_{t+n})$ and will compute $dQ / dW$ according to the form of the approximator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1: Basics\n",
    "\n",
    "(30 / 100 points)\n",
    "\n",
    "We want to approximate the state-action values. That is, the expected return when applying action $a_t$ in state $s_t$. Linear function approximates state-action value function by the inner product between a parameter matrix $W$ and the input state vector $s$:\n",
    "\n",
    "$$v(s, W) = W^T s$$\n",
    "\n",
    "Note that $W\\in \\mathbb R^{(O, A)}$ and $s \\in \\mathbb R^{(O, 1)}$, wherein O is the observation (state) dimensions, namely the `self.obs_dim` and A is the action dimension, namely the `self.act_dim`. Each action corresponding to one state-action values $Q(s, a)$.\n",
    "\n",
    "Note that you should finish this section **purely by Numpy without calling any other packages**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Build the algorithm-specify config.\n",
    "linear_approximator_config = merge_config(dict(\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    n=3,\n",
    "), default_config)\n",
    "\n",
    "\n",
    "class LinearTrainer(AbstractTrainer):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, linear_approximator_config)\n",
    "\n",
    "        # Initialize the abstract class.\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.n = self.config[\"n\"]\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        # [TODO] Initialize self.parameters, which is a two dimensional matrix,\n",
    "        #  and subjects to a normal distribution with scale\n",
    "        #  config[\"parameter_std\"].\n",
    "        std = self.config[\"parameter_std\"]\n",
    "        self.parameters = np.random.randn(self.obs_dim, self.act_dim) * std\n",
    "        \n",
    "        print(\"Initialize parameters with shape: {}.\".format(\n",
    "            self.parameters.shape))\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        assert processed_state.ndim == 1, processed_state.shape\n",
    "        \n",
    "        # [TODO] Compute the value for each potential action. Note that you\n",
    "        #  should NOT preprocess the state here.\"\"\"\n",
    "        return self.parameters.T @ processed_state\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Please implement the n-step Sarsa algorithm presented in Chapter 10.2\n",
    "        of the textbook. You algorithm should reduce the convention one-step\n",
    "        Sarsa when n = 1. That is:\n",
    "            TD = r_t + gamma * Q(s_t+1, a_t+1) - Q(s_t, a_t)\n",
    "            Q(s_t, a_t) = Q(s_t, a_t) + learning_rate * TD\n",
    "        \"\"\"\n",
    "        s = self.env.reset()\n",
    "        processed_s = self.process_state(s)\n",
    "        processed_states = [processed_s]\n",
    "        rewards = [0.0]\n",
    "        actions = [self.compute_action(processed_s)]\n",
    "        T = float(\"inf\")\n",
    "\n",
    "        for t in range(self.max_episode_length):\n",
    "            if t < T:\n",
    "                # [TODO]  When the termination is not reach, apply action,\n",
    "                #  process state, record state / reward / action to the\n",
    "                #  lists defined above, and deal with termination.\n",
    "                s, reward, done, _ = self.env.step(actions[t])\n",
    "                processed_states.append(self.process_state(s))\n",
    "                rewards.append(reward)\n",
    "                if done:\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    actions.append(self.compute_action(processed_states[t + 1]))\n",
    "\n",
    "            tau = t - self.n + 1\n",
    "            if tau >= 0:\n",
    "                gradient = self.compute_gradient(\n",
    "                    processed_states, actions, rewards, tau, T\n",
    "                )\n",
    "                self.apply_gradient(gradient)\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "\n",
    "    def compute_gradient(self, processed_states, actions, rewards, tau, T):\n",
    "        \"\"\"Compute the gradient\"\"\"\n",
    "        n = self.n\n",
    "\n",
    "        # [TODO] Compute the approximation goal, the truth state action value\n",
    "        #  G. It is a n-step discounted sum of rewards. Refer to Chapter 10.2\n",
    "        #  of the textbook.\n",
    "        gammas = np.array([self.gamma ** (i - tau - 1) for i in range(tau + 1, min(tau + n, T) + 1)])\n",
    "        rewards_ = np.array(rewards)[tau + 1 : min(tau + n, T) + 1]\n",
    "        G = np.sum(gammas * rewards_)\n",
    "    \n",
    "        if tau + n < T:\n",
    "            G += self.gamma ** n * self.compute_values(processed_states[tau + n])[actions[tau + n]]\n",
    "        \n",
    "        # Denote the state-action value function Q, then the loss of\n",
    "        # prediction error w.r.t. the weights can be separated into two\n",
    "        # parts (the chain rule):\n",
    "        #     dLoss / dweight = (dLoss / dQ) * (dQ / dweight)\n",
    "        # We call the first one loss_grad, and the latter one\n",
    "        # value_grad. We consider the Mean Square Error between the target\n",
    "        # value (G) and the predicted value (Q(s_t, a_t)) to be the loss.\n",
    "\n",
    "        loss_grad = np.zeros((self.act_dim, 1))\n",
    "        # [TODO] fill the propoer value of loss_grad, denoting the gradient\n",
    "        # of the MSE w.r.t. the output of the linear function.\n",
    "        # Hint: only one element of loss_grad is not zero.\n",
    "        loss_grad[actions[tau]] = G - self.compute_values(processed_states[tau])[actions[tau]]\n",
    "\n",
    "        # [TODO] compute the value of value_grad, denoting the gradient of\n",
    "        # the output of the linear function w.r.t. the parameters.\n",
    "        value_grad = np.array(processed_states[tau]).reshape((self.obs_dim, 1))\n",
    "\n",
    "        assert loss_grad.shape == (self.act_dim, 1)\n",
    "        assert value_grad.shape == (self.obs_dim, 1)\n",
    "\n",
    "        # [TODO] merge two gradients to get the gradient of loss w.r.t. the\n",
    "        # parameters.\n",
    "        gradient = value_grad @ loss_grad.T\n",
    "    \n",
    "        return gradient\n",
    "\n",
    "    def apply_gradient(self, gradient):\n",
    "        \"\"\"Apply the gradient to the parameter.\"\"\"\n",
    "        assert gradient.shape == self.parameters.shape, (\n",
    "            gradient.shape, self.parameters.shape)\n",
    "        # [TODO] apply the gradient to self.parameters\n",
    "        self.parameters += self.learning_rate * gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initialize parameters with shape: (4, 2).\nNow your codes should be bug-free.\n"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Build the test trainer.\n",
    "test_trainer = LinearTrainer(dict(parameter_std=0.0))\n",
    "\n",
    "# Test self.parameters.\n",
    "assert test_trainer.parameters.std() == 0.0, \\\n",
    "    \"Parameters should subjects to a normal distribution with standard \" \\\n",
    "    \"deviation config['parameter_std'], but you have {}.\" \\\n",
    "    \"\".format(test_trainer.parameters.std())\n",
    "assert test_trainer.parameters.mean() == 0, \\\n",
    "    \"Parameters should subjects to a normal distribution with mean 0. \" \\\n",
    "    \"But you have {}.\".format(test_trainer.parameters.mean())\n",
    "\n",
    "# Test compute_values\n",
    "fake_state = test_trainer.env.observation_space.sample()\n",
    "processed_state = test_trainer.process_state(fake_state)\n",
    "assert processed_state.shape == (test_trainer.obs_dim, ), processed_state.shape\n",
    "values = test_trainer.compute_values(fake_state)\n",
    "assert values.shape == (test_trainer.act_dim, ), values.shape\n",
    "\n",
    "# Test compute_gradient\n",
    "tmp_gradient = test_trainer.compute_gradient(\n",
    "    [processed_state]*10, [test_trainer.env.action_space.sample()]*10, [0.0]*10, 2, 5)\n",
    "assert tmp_gradient.shape == test_trainer.parameters.shape\n",
    "\n",
    "test_trainer.train()\n",
    "print(\"Now your codes should be bug-free.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initialize parameters with shape: (4, 2).\n(0.0s,+0.0s)\tIteration 0, current mean episode reward is 9.18. \n(0.8s,+0.8s)\tIteration 1000, current mean episode reward is 9.62. \n(1.6s,+0.8s)\tIteration 2000, current mean episode reward is 9.72. \n(2.4s,+0.8s)\tIteration 3000, current mean episode reward is 9.78. \n(3.3s,+0.9s)\tIteration 4000, current mean episode reward is 9.78. \n(4.2s,+0.9s)\tIteration 5000, current mean episode reward is 9.84. \n(5.1s,+0.9s)\tIteration 6000, current mean episode reward is 9.84. \n(6.0s,+0.9s)\tIteration 7000, current mean episode reward is 9.82. \n(7.0s,+1.0s)\tIteration 8000, current mean episode reward is 9.84. \n(9.6s,+2.6s)\tIteration 9000, current mean episode reward is 127.54. \n(16.3s,+6.7s)\tIteration 10000, current mean episode reward is 113.5. \n"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "linear_trainer, _ = run(LinearTrainer, dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=1000, \n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    n=3,\n",
    "    env_name=\"CartPole-v0\"\n",
    "))\n",
    "\n",
    "# It's OK to see bad performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Average episode reward for your linear agent in CartPole-v0:  160.0\n"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# You should see a pop up window which display the movement of the cart and pole.\n",
    "print(\"Average episode reward for your linear agent in CartPole-v0: \",\n",
    "      linear_trainer.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the linear trainer only has 8 trainable parameters and its performance is quiet bad. In the following section, we will expand the size of parameters and introduce more features as the input to the system so that the system can learn more complex value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2: Linear Model with Feature Construction\n",
    "\n",
    "(15 / 100 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "linear_fc_config = merge_config(dict(\n",
    "    polynomial_order=1,\n",
    "), linear_approximator_config)\n",
    "\n",
    "\n",
    "def polynomial_feature(sequence, order=1):\n",
    "    \"\"\"Construct the order-n polynomial-basis feature of the state.\n",
    "    Refer to Chapter 9.5.1 of the textbook. We expect to get a \n",
    "    vector of length (n+1)^k as the output.\n",
    "    Example:\n",
    "    When the state is [2, 3, 4], the first order polynomial feature\n",
    "    of the state is [\n",
    "        1,\n",
    "        2,\n",
    "        3,\n",
    "        4,\n",
    "        2 * 3 = 6,\n",
    "        2 * 4 = 8,\n",
    "        3 * 4 = 12,\n",
    "        2 * 3 * 4 = 24\n",
    "    ]\n",
    "    It's OK for function polynomial() to return values in different order.\n",
    "    \"\"\"\n",
    "    # [TODO] finish this function.\n",
    "    if len(sequence)== 0:\n",
    "        return [1]\n",
    "    ret = polynomial_feature(sequence[:-1], order)\n",
    "    l = []\n",
    "    for i in range(1, order + 1):\n",
    "        for x in ret:\n",
    "            l.append(x * sequence[-1] ** i)\n",
    "    return ret + l\n",
    "\n",
    "assert sorted(polynomial_feature([2, 3, 4])) == [1, 2, 3, 4, 6, 8, 12, 24]\n",
    "assert len(polynomial_feature([2, 3, 4], 2)) == 27\n",
    "assert len(polynomial_feature([2, 3, 4], 3)) == 64\n",
    "\n",
    "class LinearTrainerWithFeatureConstruction(LinearTrainer):\n",
    "    \"\"\"In this class, we will expand the dimension of the state.\n",
    "    This procedure is done at self.process_state function.\n",
    "    The modification of self.obs_dim and the shape of parameters\n",
    "    is also needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, linear_fc_config)\n",
    "        # Initialize the abstract class.\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.polynomial_order = self.config[\"polynomial_order\"]\n",
    "\n",
    "        # Expand the size of observation\n",
    "        self.obs_dim = (self.polynomial_order + 1) ** self.obs_dim\n",
    "\n",
    "        # Since we change self.obs_dim, reset the parameters.\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def process_state(self, state):\n",
    "        \"\"\"Please finish the polynomial function.\"\"\"\n",
    "        processed = polynomial_feature(state, self.polynomial_order)\n",
    "        processed = np.asarray(processed)\n",
    "        assert len(processed) == self.obs_dim, processed.shape\n",
    "        return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initialize parameters with shape: (4, 2).\nInitialize parameters with shape: (16, 2).\n(0.0s,+0.0s)\tIteration 0, current mean episode reward is 9.24. \n(1.5s,+1.4s)\tIteration 1000, current mean episode reward is 9.9. \n(3.0s,+1.5s)\tIteration 2000, current mean episode reward is 12.52. \n(4.9s,+1.9s)\tIteration 3000, current mean episode reward is 14.8. \n(7.4s,+2.5s)\tIteration 4000, current mean episode reward is 23.5. \n(11.9s,+4.4s)\tIteration 5000, current mean episode reward is 61.84. \n(16.1s,+4.3s)\tIteration 6000, current mean episode reward is 54.92. \n(20.8s,+4.6s)\tIteration 7000, current mean episode reward is 75.44. \n(25.1s,+4.4s)\tIteration 8000, current mean episode reward is 65.8. \n(29.4s,+4.2s)\tIteration 9000, current mean episode reward is 33.14. \n(34.2s,+4.8s)\tIteration 10000, current mean episode reward is 64.82. \n"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "linear_fc_trainer, _ = run(LinearTrainerWithFeatureConstruction, dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=1000, \n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.001,\n",
    "    polynomial_order=1,\n",
    "    n=3,\n",
    "    env_name=\"CartPole-v0\"\n",
    "), reward_threshold=195.0)\n",
    "\n",
    "assert linear_fc_trainer.evaluate() > 20.0, \"The best episode reward happening \" \\\n",
    "    \"during training should be greater than the random baseline, that is greather than 20+.\"\n",
    "\n",
    "# This cell should be finished within 10 minitines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Average episode reward for your linear agent with feature constructioin in CartPole-v0:  43.0\n"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# You should see a pop up window which display the movement of the cart and pole.\n",
    "print(\"Average episode reward for your linear agent with feature constructioin in CartPole-v0: \",\n",
    "      linear_fc_trainer.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Multi-layer Perceptron as the approximiator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are required to implement a single agent MLP using purely Numpy package. The differences between MLP and linear function are (1). MLP has a hidden layer which increase its representation capacity (2). MLP can leverage activation function after the output of each layer which introduce not linearity.\n",
    "\n",
    "Consider a MLP with one hidden layer containing 100 neurons and activation function `f()`. We call the layer that accepts the state as input and output the activation **hidden layer**, and the layer that accepts the activation as input and produces the values **output layer**. The activation of the hidden layer is:\n",
    "\n",
    "$$a(s_t) = f( W_h^T s_t)$$\n",
    "\n",
    "obvious the activation is a 100-length vector. The output values is:\n",
    "\n",
    "$$Q(s_t) = f(W_o ^ T a(s_t))$$\n",
    "\n",
    "wherein $W_h, W_o$ are the parameters of hidden layer and output layer, respectively. In this section we do not add activation function and hence $f(x) = x$.\n",
    "\n",
    "Moreover, we also introduce the gradient clipping mechanism. In on-policy learning, the norm of gradient is prone to vary drastically, since the output of Q function is unbounded and it can be as large as possible, which leads to exploding gradient issue. Gradient clipping is used to bound the norm of gradient while keeps the direction of gradient vector unchanged. Concretely, the formulation of gradient clipping is:\n",
    "\n",
    "$$g_{clipped} = g_{original} \\cfrac{c}{\\max(c, \\text{norm}(g))}$$\n",
    "\n",
    "wherein $c$ is a hyperparameter which is `config[\"clip_norm\"]` in our implementation. Gradient clipping bounds the gradient norm to $c$ if the norm of original gradient is greater than $c$. You need to implement this mechanism in function `apply_gradient` in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Build the algorithm-specify config.\n",
    "mlp_trainer_config = merge_config(dict(\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    hidden_dim=100,\n",
    "    n=3,\n",
    "    clip_norm=1.0,\n",
    "    clip_gradient=True\n",
    "), default_config)\n",
    "\n",
    "\n",
    "class MLPTrainer(LinearTrainer):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, mlp_trainer_config)\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        super().__init__(config)\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        # [TODO] Initialize self.hidden_parameters and self.output_parameters,\n",
    "        #  which are two dimensional matrices, and subject to normal\n",
    "        #  distributions with scale config[\"parameter_std\"]\n",
    "        std = self.config[\"parameter_std\"]\n",
    "        self.hidden_parameters = std * np.random.randn(self.obs_dim, self.hidden_dim)\n",
    "        self.output_parameters = std * np.random.randn(self.hidden_dim, self.act_dim)\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        \"\"\"Compute the value for each potential action. Note that you\n",
    "        should NOT preprocess the state here.\"\"\"\n",
    "        assert processed_state.ndim == 1, processed_state.shape\n",
    "        activation = self.compute_activation(processed_state)\n",
    "        values = self.output_parameters.T @ activation\n",
    "        return values\n",
    "\n",
    "    def compute_activation(self, processed_state):\n",
    "        \"\"\"Given a processed state, first we need to compute the activtaion\n",
    "        (the output of hidden layer). Then we compute the values (the output of\n",
    "        the output layer).\n",
    "        \"\"\"\n",
    "        activation = self.hidden_parameters.T @ processed_state\n",
    "        return activation\n",
    "\n",
    "    def compute_gradient(self, processed_states, actions, rewards, tau, T):\n",
    "        n = self.n\n",
    "        \n",
    "        # [TODO] compute the target value.\n",
    "        # Hint: copy your codes in LinearTrainer.\n",
    "        gammas = np.array([self.gamma ** (i - tau - 1) for i in range(tau + 1, min(tau + n, T) + 1)])\n",
    "        rewards_ = np.array(rewards)[tau + 1 : min(tau + n, T) + 1]\n",
    "        G = np.sum(gammas * rewards_)\n",
    "\n",
    "        if tau + n < T:\n",
    "            # Hint: Since we use Sarsa algorithm here,\n",
    "            #  the Q value of time tau+n is the Q value of action\n",
    "            #  in time tau+n. So you should take the tau+n element of\n",
    "            #  processed_states as input to compute the Q values\n",
    "            #  and then take the \"actions[tau+n]\" as the index to get\n",
    "            #  the Q value in tau+n.\n",
    "            Q_tau_plus_n = self.compute_values(processed_states[tau + n])[actions[tau + n]]   \n",
    "            G = G + (self.gamma ** n) * Q_tau_plus_n\n",
    "\n",
    "        # Denote the state-action value function Q, then the loss of\n",
    "        # prediction error w.r.t. the output layer weights can be \n",
    "        # separated into two parts (the chain rule):\n",
    "        #     dError / dweight = (dError / dQ) * (dQ / dweight)\n",
    "        # We call the first one loss_grad, and the latter one\n",
    "        # value_grad. We consider the Mean Square Error between the target\n",
    "        # value (G) and the predict value (Q(s_t, a_t)) to be the loss.\n",
    "        cur_state = processed_states[tau]\n",
    "\n",
    "        loss_grad = np.zeros((self.act_dim, 1))  # [act_dim, 1]\n",
    "        # [TODO] compute loss_grad\n",
    "        loss_grad[actions[tau]] = G - self.compute_values(processed_states[tau])[actions[tau]]\n",
    "        \n",
    "        # [TODO] compute the gradient of output layer parameters\n",
    "        output_gradient = self.hidden_parameters.T @ np.array(processed_states[tau]).reshape((-1,1)) @ loss_grad.T\n",
    "        \n",
    "        # [TODO] compute the gradient of hidden layer parameters\n",
    "        # Hint: using chain rule and derive the formulation\n",
    "        hidden_gradient = np.array(processed_states[tau]).reshape((-1,1)) @ loss_grad.T @ self.output_parameters.T\n",
    "    \n",
    "        assert np.all(np.isfinite(output_gradient)), \\\n",
    "            \"Invalid value occurs in output_gradient! {}\".format(\n",
    "                output_gradient)\n",
    "        assert np.all(np.isfinite(hidden_gradient)), \\\n",
    "            \"Invalid value occurs in hidden_gradient! {}\".format(\n",
    "                hidden_gradient)\n",
    "        return [hidden_gradient, output_gradient]\n",
    "\n",
    "    def apply_gradient(self, gradients):\n",
    "        \"\"\"Apply the gradientss to the two layers' parameters.\"\"\"\n",
    "        assert len(gradients) == 2\n",
    "        hidden_gradient, output_gradient = gradients\n",
    "\n",
    "        assert output_gradient.shape == (self.hidden_dim, self.act_dim)\n",
    "        assert hidden_gradient.shape == (self.obs_dim, self.hidden_dim)\n",
    "        \n",
    "        # [TODO] Implement the clip gradient mechansim\n",
    "        # Hint: when the old gradient has norm less that clip_norm,\n",
    "        #  then nothing happens. Otherwise shrink the gradient to\n",
    "        #  make its norm equal to clip_norm.\n",
    "        if self.config[\"clip_gradient\"]:\n",
    "            clip_norm = self.config[\"clip_norm\"]\n",
    "            og_norm = np.linalg.norm(output_gradient)\n",
    "            if og_norm > clip_norm:\n",
    "                output_gradient = output_gradient * clip_norm / og_norm\n",
    "            hg_norm = np.linalg.norm(hidden_gradient)\n",
    "            if hg_norm > clip_norm:\n",
    "                hidden_gradient = hidden_gradient * clip_norm / hg_norm\n",
    "\n",
    "        # [TODO] update the parameters\n",
    "        # Hint: Remember to check the sign when applying the gradient\n",
    "        #  into the parameters. Should you add or minus the gradients?\n",
    "        self.hidden_parameters += self.learning_rate * hidden_gradient\n",
    "        self.output_parameters += self.learning_rate * output_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Now let's see what happen if clip gradient is not enable!\n(0.3s,+0.3s)\tIteration 0, current mean episode reward is 34.82. \n(1.3s,+1.1s)\tIteration 100, current mean episode reward is 92.88. \n(2.4s,+1.0s)\tIteration 200, current mean episode reward is 90.86. \n(3.4s,+1.0s)\tIteration 300, current mean episode reward is 81.94. \n(4.3s,+0.9s)\tIteration 400, current mean episode reward is 77.46. \n(5.1s,+0.8s)\tIteration 500, current mean episode reward is 64.52. \n(5.9s,+0.8s)\tIteration 600, current mean episode reward is 60.56. \n(6.6s,+0.8s)\tIteration 700, current mean episode reward is 60.5. \n(7.3s,+0.6s)\tIteration 800, current mean episode reward is 52.86. \n(7.9s,+0.6s)\tIteration 900, current mean episode reward is 53.28. \n(8.5s,+0.6s)\tIteration 1000, current mean episode reward is 48.24. \n(9.1s,+0.6s)\tIteration 1100, current mean episode reward is 47.32. \n(9.6s,+0.5s)\tIteration 1200, current mean episode reward is 44.8. \n(10.1s,+0.5s)\tIteration 1300, current mean episode reward is 44.6. \n(10.7s,+0.5s)\tIteration 1400, current mean episode reward is 46.82. \n(11.2s,+0.5s)\tIteration 1500, current mean episode reward is 43.22. \n(11.7s,+0.5s)\tIteration 1600, current mean episode reward is 39.62. \n(12.2s,+0.5s)\tIteration 1700, current mean episode reward is 40.38. \n(12.7s,+0.5s)\tIteration 1800, current mean episode reward is 41.24. \n(13.2s,+0.5s)\tIteration 1900, current mean episode reward is 40.72. \n(13.7s,+0.5s)\tIteration 2000, current mean episode reward is 41.96. \n(14.2s,+0.5s)\tIteration 2100, current mean episode reward is 39.56. \n(14.7s,+0.4s)\tIteration 2200, current mean episode reward is 34.44. \n(15.1s,+0.4s)\tIteration 2300, current mean episode reward is 33.58. \n(15.5s,+0.4s)\tIteration 2400, current mean episode reward is 33.28. \n(15.9s,+0.4s)\tIteration 2500, current mean episode reward is 33.28. \n(16.3s,+0.4s)\tIteration 2600, current mean episode reward is 33.58. \n(16.8s,+0.5s)\tIteration 2700, current mean episode reward is 33.58. \n(17.2s,+0.4s)\tIteration 2800, current mean episode reward is 33.58. \n(17.6s,+0.4s)\tIteration 2900, current mean episode reward is 33.0. \n(18.0s,+0.4s)\tIteration 3000, current mean episode reward is 31.98. \nWe expect to see bad performance (<195). The performance without gradient clipping: 31.98.\nTry next cell to see the impact of gradient clipping.\n"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "print(\"Now let's see what happen if clip gradient is not enable!\")\n",
    "try:\n",
    "    failed_mlp_trainer, _ = run(MLPTrainer, dict(\n",
    "        max_iteration=3000,\n",
    "        evaluate_interval=100, \n",
    "        parameter_std=0.01,\n",
    "        learning_rate=0.001,\n",
    "        hidden_dim=100,\n",
    "        clip_gradient=False,  # <<< Gradient clipping is OFF!\n",
    "        env_name=\"CartPole-v0\"\n",
    "    ), reward_threshold=195.0)\n",
    "    print(\"We expect to see bad performance (<195). \"\n",
    "          \"The performance without gradient clipping: {}.\"\n",
    "          \"\".format(failed_mlp_trainer.evaluate()))\n",
    "except AssertionError as e:\n",
    "    print(traceback.format_exc())\n",
    "    print(\"Infinity happen during training. It's OK since the gradient is not bounded.\")\n",
    "finally:\n",
    "    print(\"Try next cell to see the impact of gradient clipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Now let's see what happen if clip gradient is not enable!\n(0.1s,+0.1s)\tIteration 0, current mean episode reward is 34.92. \n(1.1s,+1.1s)\tIteration 100, current mean episode reward is 91.5. \n(2.3s,+1.2s)\tIteration 200, current mean episode reward is 93.26. \n(3.6s,+1.2s)\tIteration 300, current mean episode reward is 99.64. \n(4.8s,+1.2s)\tIteration 400, current mean episode reward is 102.56. \n(6.0s,+1.2s)\tIteration 500, current mean episode reward is 106.38. \n(7.1s,+1.2s)\tIteration 600, current mean episode reward is 110.38. \n(8.4s,+1.3s)\tIteration 700, current mean episode reward is 118.38. \n(9.8s,+1.4s)\tIteration 800, current mean episode reward is 121.64. \n(11.3s,+1.5s)\tIteration 900, current mean episode reward is 166.64. \n(13.2s,+1.8s)\tIteration 1000, current mean episode reward is 187.38. \n(15.3s,+2.1s)\tIteration 1100, current mean episode reward is 199.04. \nIn 1100 iteration, current mean episode reward 199.040 is greater than reward threshold 195.0. Congratulation! Now we exit the training process.\n"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "print(\"Now let's see what happen if clip gradient is not enable!\")\n",
    "mlp_trainer, _ = run(MLPTrainer, dict(\n",
    "    max_iteration=3000,\n",
    "    evaluate_interval=100, \n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.001,\n",
    "    hidden_dim=100,\n",
    "    clip_gradient=True,  # <<< Gradient clipping is ON!\n",
    "    env_name=\"CartPole-v0\"\n",
    "), reward_threshold=195.0)\n",
    "\n",
    "assert mlp_trainer.evaluate() > 195.0, \"Check your codes. \" \\\n",
    "    \"Your agent should achieve {} reward in 200 iterations.\" \\\n",
    "    \"But it achieve {} reward in evaluation.\"\n",
    "\n",
    "# In our implementation, the task is solved in 200 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Average episode reward for your MLP agent with gradient clipping in CartPole-v0:  200.0\n"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# You should see a pop up window which display the movement of the cart and pole.\n",
    "print(\"Average episode reward for your MLP agent with gradient clipping in CartPole-v0: \",\n",
    "      mlp_trainer.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting right? The gradient clipping technique makes the training converge much faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Implement Deep Q Learning in Pytorch\n",
    "\n",
    "(50 / 100 points)\n",
    "\n",
    "In this section, you will get familiar with the basic logic of pytorch, which lay the ground for further learning. We will implement a MLP similar to the one in Section 3 using Pytorch, a powerful Deep Learning framework. Before start, you need to make sure using `pip install torch` to install it.\n",
    "\n",
    "If you are not familiar with Pytorch, we suggest you to go through pytorch official quickstart tutorials:\n",
    "1. [quickstart](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "2. [tutorial on RL](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "\n",
    "Different from the algorithm in Section 3, we will implement Deep Q Network (DQN) in this section. The main differences are concluded as following:\n",
    "\n",
    "**DQN requires an experience replay memory to store the transitions.** A replay memory is implemented in the following `ExperienceReplayMemory` class. It can contain a certain amount of transitions: `(s_t, a_t, r_t, s_t+1, done_t)`. When the memory is full, the earliest transition is discarded to store the latest one.\n",
    "\n",
    "The introduction of replay memory increase the sample efficiency (since each transition might be used multiple times) when solving complex task, though you may find it learn slowly in this assignment since the CartPole-v0 is a relatively easy environment.\n",
    "\n",
    "\n",
    "**DQN is an off-policy algorithm and has difference when computing TD error, compared to Sarsa.** In Sarsa, the TD error is computed as: \n",
    "\n",
    "$$(r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$$ \n",
    "\n",
    "wherein the next action $a_{t+1}$ is the one the policy selects. However, in traditional Q learning, it assume the next action is the one that maximizes the action values and use this assumption to compute the TD: \n",
    "\n",
    "$$(r_t + \\gamma \\max_{a_{t+1}} Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$$\n",
    "\n",
    "**DQN has make delayed update target network, which is another difference even compared to the traditional Q learning.** DQN maintains another neural network called target network that has identical structure of the Q network. After a certain amount of steps has been taken, the target network copies the parameters of the Q network to itself. Normally, the update of target network is much less frequent than the update of the Q network. The Q network is updated in each step.\n",
    "\n",
    "The reason to leverage the target network is to stabilize the estimation of TD error. In DQN, the TD error is evaluated as:\n",
    "\n",
    "$$(r_t + \\gamma \\max_{a_{t+1}} Q^{target}(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$$\n",
    "\n",
    "The Q values of next state is estimated by the target network, not the Q network that is updating. This mechanism can reduce the variance of gradient because the estimation of Q values of next states is not influenced by the update of the Q network.\n",
    "\n",
    "In the engineering aspect, the differences between `DQNTrainer` and the previous `MLPTrainer` are:\n",
    "\n",
    "1. DQN uses pytorch model to serve as the approximator. So we need to rewrite the `initialize_parameter` function to build the pytorch model. Also the `train` function is changed since the gradient optimization is conducted by pytorch, therefore we need to write the pytorch pipeline in `train`.\n",
    "2. DQN has replay memory. So we need to initialize it, feed data into it and take the transitions out.\n",
    "3. Thank to the replay memory and pytorch, DQN can be updated in a batch. So you need to carefully compute the Q target via matrix computation.\n",
    "4. We use Adam optimizer to conduct the gradient optimization. You need to get familiar with how to compute the loss and conduct backward propagation.\n",
    "\n",
    "\\* Note that in this assignment, we use purely CPU to train the agent. You do not need to right your pytorch code that compatible with GPU and also do not need to run your code in a GPU machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ExperienceReplayMemory:\n",
    "    \"\"\"Store and sample the transitions\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        # deque is a useful class which acts like a list but only contain\n",
    "        # finite elements.When appending new element make deque exceeds the \n",
    "        # `maxlen`, the oldest element (the index 0 element) will be removed.\n",
    "        \n",
    "        # [TODO] uncomment next line. \n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        pass\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class PytorchModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(PytorchModel, self).__init__()\n",
    "        \n",
    "        # [TODO] Build a sequential model with two hidden layers.\n",
    "        # The first hidden layer has 100 hidden units, followed by\n",
    "        # a ReLU activation function.\n",
    "        # The second output layer take the activation vector as input\n",
    "        # and pass through another 100 hidden units, also followed\n",
    "        # by a ReLU activation function.\n",
    "        # Then the output 100-length vector passes the output layer\n",
    "        # and output the values of all actions.\n",
    "        # \n",
    "        #     input (length: obs_dim) \n",
    "        # ->  100 units hidden layer\n",
    "        # ->  ReLU \n",
    "        # ->  100 units hidden layer\n",
    "        # ->  ReLU\n",
    "        # ->  act_dim units hidden layer\n",
    "        # -> output (length: act_dim)\n",
    "        self.action_value = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, act_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.action_value(obs)\n",
    "    \n",
    "# Test\n",
    "test_model = PytorchModel(3, 7)\n",
    "assert isinstance(test_model.action_value, nn.Module)\n",
    "assert test_model(torch.from_numpy(np.ones([3], dtype=np.float32))).shape \\\n",
    "    == torch.Size([7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "pytorch_config = merge_config(dict(\n",
    "    memory_size=50000,\n",
    "    learn_start=5000,\n",
    "    batch_size=32,\n",
    "    target_update_freq=500,  # in steps\n",
    "    learn_freq=1,  # in steps\n",
    "    n=1\n",
    "), mlp_trainer_config)\n",
    "\n",
    "\n",
    "def to_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.float32)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x\n",
    "\n",
    "\n",
    "class DQNTrainer(MLPTrainer):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, pytorch_config)\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.memory = ExperienceReplayMemory(config[\"memory_size\"])\n",
    "        self.learn_start = config[\"learn_start\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.target_update_freq = config[\"target_update_freq\"]\n",
    "        self.clip_norm = config[\"clip_norm\"]\n",
    "        self.step_since_update = 0\n",
    "        self.total_step = 0\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"Initialize the pytorch model as the Q network and the target network\"\"\"\n",
    "        # [TODO] Initialize two network using PytorchModel class\n",
    "        self.network = PytorchModel(self.obs_dim, self.act_dim)\n",
    "\n",
    "        self.network.eval()\n",
    "        self.network.share_memory()\n",
    "\n",
    "        # [TODO] Initialize target network, which is identical to self.network,\n",
    "        # and should have the same weights with self.network. So you should\n",
    "        # put the weights of self.network into self.target_network.\n",
    "        self.target_network = PytorchModel(self.obs_dim, self.act_dim)\n",
    "\n",
    "        self.target_network.eval()\n",
    "\n",
    "        # Build Adam optimizer and MSE Loss.\n",
    "        # [TODO] Uncomment next few lines\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.network.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        \"\"\"Compute the value for each potential action. Note that you\n",
    "        should NOT preprocess the state here.\"\"\"\n",
    "        # [TODO] Convert the output of neural network to numpy array\n",
    "        return self.network(processed_state).detach().numpy()\n",
    "\n",
    "    def train(self):\n",
    "        s = self.env.reset()\n",
    "        processed_s = self.process_state(s)\n",
    "        act = self.compute_action(processed_s)\n",
    "        stat = {\"loss\": []}\n",
    "\n",
    "        for t in range(self.max_episode_length):\n",
    "            next_state, reward, done, _ = self.env.step(act)\n",
    "            next_processed_s = self.process_state(next_state)\n",
    "\n",
    "            # Push the transition into memory.\n",
    "            self.memory.push(\n",
    "                (processed_s, act, reward, next_processed_s, done)\n",
    "            )\n",
    "\n",
    "            processed_s = next_processed_s\n",
    "            act = self.compute_action(next_processed_s)\n",
    "            self.step_since_update += 1\n",
    "            self.total_step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if t % self.config[\"learn_freq\"] != 0:\n",
    "                # It's not necessary to update in each step.\n",
    "                continue\n",
    "\n",
    "            if len(self.memory) < self.learn_start:\n",
    "                continue\n",
    "            elif len(self.memory) == self.learn_start:\n",
    "                print(\"Current memory contains {} transitions, \"\n",
    "                      \"start learning!\".format(self.learn_start))\n",
    "\n",
    "            batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "            # Transform a batch of state / action / .. into a tensor.\n",
    "            state_batch = to_tensor(\n",
    "                np.stack([transition[0] for transition in batch])\n",
    "            )\n",
    "            action_batch = to_tensor(\n",
    "                np.stack([transition[1] for transition in batch])\n",
    "            )\n",
    "            reward_batch = to_tensor(\n",
    "                np.stack([transition[2] for transition in batch])\n",
    "            )\n",
    "            next_state_batch = torch.stack(\n",
    "                [transition[3] for transition in batch]\n",
    "            )\n",
    "            done_batch = to_tensor(\n",
    "                np.stack([transition[4] for transition in batch])\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # [TODO] Compute the values of Q in next state in batch.\n",
    "                # Hint: \n",
    "                #  1. Q_t_plus_one is the maximum value of Q values of possible\n",
    "                #     actions in next state. So the input to the network is \n",
    "                #     next_state_batch.\n",
    "                #  2. Q_t_plus_one is computed using the target network.\n",
    "                Q_t_plus_one, _ = torch.max(self.target_network(next_state_batch), dim=1)\n",
    "                \n",
    "                assert isinstance(Q_t_plus_one, torch.Tensor)\n",
    "                assert Q_t_plus_one.dim() == 1\n",
    "                \n",
    "                # [TODO] Compute the target value of Q in batch.\n",
    "                # Hint: The Q target is simply r_t + gamma * Q_t+1 \n",
    "                #  IF the episode is not done at time t.\n",
    "                #  That is, the (gamma*Q_t+1) term should be masked out\n",
    "                #  if done_batch[t] is True.\n",
    "                #  A smart way to do so is: using (1-done_batch) as multiplier\n",
    "                Q_target = reward_batch + (1 - done_batch) * self.gamma * Q_t_plus_one\n",
    "                Q_target = Q_target.reshape((-1,))\n",
    "                \n",
    "                assert Q_target.shape == (self.batch_size,)\n",
    "            \n",
    "            # [TODO] Collect the Q values in batch.\n",
    "            # Hint: Remember to call self.network.train()\n",
    "            #  before you get the Q value from self.network(state_batch),\n",
    "            #  otherwise the graident will not be recorded by pytorch.\n",
    "            self.network.train()\n",
    "            Q_t = self.network(state_batch)[torch.arange(self.batch_size, dtype=torch.long), action_batch.type(torch.long)]\n",
    "            Q_t = Q_t.reshape((-1,))\n",
    "\n",
    "            assert Q_t.shape == Q_target.shape\n",
    "\n",
    "            # Update the network\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss(input=Q_t, target=Q_target)\n",
    "            loss_value = loss.item()\n",
    "            stat['loss'].append(loss_value)\n",
    "            loss.backward()\n",
    "            \n",
    "            # [TODO] Gradient clipping. Uncomment next line\n",
    "            nn.utils.clip_grad_norm_(self.network.parameters(), self.clip_norm)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            self.network.eval()\n",
    "\n",
    "        if len(self.memory) >= self.learn_start and \\\n",
    "                self.step_since_update > self.target_update_freq:\n",
    "            print(\"{} steps has passed since last update. Now update the\"\n",
    "                  \" parameter of the behavior policy. Current step: {}\".format(\n",
    "                self.step_since_update, self.total_step\n",
    "            ))\n",
    "            self.step_since_update = 0\n",
    "            # [TODO] Copy the weights of self.network to self.target_network.\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "            \n",
    "            self.target_network.eval()\n",
    "            \n",
    "        return {\"loss\": np.mean(stat[\"loss\"]), \"episode_len\": t}\n",
    "\n",
    "    def process_state(self, state):\n",
    "        return torch.from_numpy(state).type(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Now your codes should be bug-free.\n(0.2s,+0.2s)\tIteration 0, current mean episode reward is 9.48. {'loss': nan, 'episode_len': 8.0}\nCurrent memory contains 100 transitions, start learning!\n(0.5s,+0.3s)\tIteration 10, current mean episode reward is 9.64. {'loss': 0.0053, 'episode_len': 10.0}\n(1.1s,+0.6s)\tIteration 20, current mean episode reward is 9.48. {'loss': 0.0001, 'episode_len': 19.0}\nTest passed!\n"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Build the test trainer.\n",
    "test_trainer = DQNTrainer({})\n",
    "\n",
    "# Test compute_values\n",
    "fake_state = test_trainer.env.observation_space.sample()\n",
    "processed_state = test_trainer.process_state(fake_state)\n",
    "assert processed_state.shape == (test_trainer.obs_dim, ), processed_state.shape\n",
    "values = test_trainer.compute_values(processed_state)\n",
    "assert values.shape == (test_trainer.act_dim, ), values.shape\n",
    "\n",
    "test_trainer.train()\n",
    "print(\"Now your codes should be bug-free.\")\n",
    "\n",
    "_ = run(DQNTrainer, dict(\n",
    "    max_iteration=20,\n",
    "    evaluate_interval=10, \n",
    "    learn_start=100,\n",
    "    env_name=\"CartPole-v0\",\n",
    "))\n",
    "\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(0.2s,+0.2s)\tIteration 0, current mean episode reward is 9.48. {'loss': nan, 'episode_len': 8.0}\n(0.4s,+0.2s)\tIteration 10, current mean episode reward is 9.48. {'loss': nan, 'episode_len': 10.0}\n(0.5s,+0.2s)\tIteration 20, current mean episode reward is 9.48. {'loss': nan, 'episode_len': 12.0}\n(0.8s,+0.2s)\tIteration 30, current mean episode reward is 9.48. {'loss': nan, 'episode_len': 9.0}\n(0.9s,+0.2s)\tIteration 40, current mean episode reward is 9.48. {'loss': nan, 'episode_len': 8.0}\n(1.1s,+0.2s)\tIteration 50, current mean episode reward is 9.48. {'loss': nan, 'episode_len': 8.0}\n(1.3s,+0.2s)\tIteration 60, current mean episode reward is 9.48. {'loss': nan, 'episode_len': 9.0}\n(1.4s,+0.2s)\tIteration 70, current mean episode reward is 9.48. {'loss': nan, 'episode_len': 9.0}\n(1.6s,+0.2s)\tIteration 80, current mean episode reward is 9.48. {'loss': nan, 'episode_len': 9.0}\n(1.8s,+0.2s)\tIteration 90, current mean episode reward is 9.48. {'loss': nan, 'episode_len': 13.0}\nCurrent memory contains 1000 transitions, start learning!\n1005 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 1005\n(2.2s,+0.4s)\tIteration 100, current mean episode reward is 25.1. {'loss': 0.3626, 'episode_len': 8.0}\n(2.7s,+0.5s)\tIteration 110, current mean episode reward is 9.48. {'loss': 0.109, 'episode_len': 9.0}\n(3.2s,+0.4s)\tIteration 120, current mean episode reward is 9.24. {'loss': 0.0781, 'episode_len': 9.0}\n(3.6s,+0.4s)\tIteration 130, current mean episode reward is 9.24. {'loss': 0.1193, 'episode_len': 10.0}\n(4.0s,+0.4s)\tIteration 140, current mean episode reward is 9.34. {'loss': 0.0721, 'episode_len': 9.0}\n(4.4s,+0.4s)\tIteration 150, current mean episode reward is 9.6. {'loss': 0.0874, 'episode_len': 12.0}\n(4.8s,+0.4s)\tIteration 160, current mean episode reward is 9.48. {'loss': 0.0691, 'episode_len': 8.0}\n(5.2s,+0.4s)\tIteration 170, current mean episode reward is 9.46. {'loss': 0.0417, 'episode_len': 8.0}\n(5.6s,+0.4s)\tIteration 180, current mean episode reward is 9.46. {'loss': 0.0671, 'episode_len': 10.0}\n(6.1s,+0.5s)\tIteration 190, current mean episode reward is 9.24. {'loss': 0.0598, 'episode_len': 9.0}\n1007 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 2012\n(6.5s,+0.5s)\tIteration 200, current mean episode reward is 9.48. {'loss': 0.4175, 'episode_len': 8.0}\n(7.0s,+0.4s)\tIteration 210, current mean episode reward is 9.24. {'loss': 0.0293, 'episode_len': 9.0}\n(7.5s,+0.5s)\tIteration 220, current mean episode reward is 9.42. {'loss': 0.0335, 'episode_len': 9.0}\n(8.0s,+0.5s)\tIteration 230, current mean episode reward is 10.58. {'loss': 0.0204, 'episode_len': 9.0}\n(8.5s,+0.4s)\tIteration 240, current mean episode reward is 9.52. {'loss': 0.0435, 'episode_len': 8.0}\n(8.9s,+0.5s)\tIteration 250, current mean episode reward is 10.0. {'loss': 0.0222, 'episode_len': 12.0}\n(9.4s,+0.5s)\tIteration 260, current mean episode reward is 9.68. {'loss': 0.0294, 'episode_len': 10.0}\n(9.9s,+0.5s)\tIteration 270, current mean episode reward is 10.62. {'loss': 0.0235, 'episode_len': 8.0}\n(10.3s,+0.5s)\tIteration 280, current mean episode reward is 9.48. {'loss': 0.0285, 'episode_len': 11.0}\n(10.8s,+0.5s)\tIteration 290, current mean episode reward is 10.36. {'loss': 0.036, 'episode_len': 7.0}\n1011 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 3023\n(11.4s,+0.5s)\tIteration 300, current mean episode reward is 9.74. {'loss': 0.0613, 'episode_len': 8.0}\n(11.9s,+0.5s)\tIteration 310, current mean episode reward is 10.38. {'loss': 0.0282, 'episode_len': 11.0}\n(12.4s,+0.5s)\tIteration 320, current mean episode reward is 13.48. {'loss': 0.0234, 'episode_len': 9.0}\n(12.9s,+0.5s)\tIteration 330, current mean episode reward is 10.82. {'loss': 0.0596, 'episode_len': 10.0}\n(13.5s,+0.7s)\tIteration 340, current mean episode reward is 16.82. {'loss': 0.0318, 'episode_len': 10.0}\n(14.2s,+0.6s)\tIteration 350, current mean episode reward is 11.64. {'loss': 0.0289, 'episode_len': 11.0}\n(14.7s,+0.6s)\tIteration 360, current mean episode reward is 10.5. {'loss': 0.0242, 'episode_len': 10.0}\n(15.3s,+0.6s)\tIteration 370, current mean episode reward is 11.42. {'loss': 0.0308, 'episode_len': 9.0}\n1003 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 4026\n(16.0s,+0.6s)\tIteration 380, current mean episode reward is 15.02. {'loss': 0.0438, 'episode_len': 9.0}\n(16.9s,+0.9s)\tIteration 390, current mean episode reward is 17.56. {'loss': 0.044, 'episode_len': 28.0}\n(17.9s,+1.0s)\tIteration 400, current mean episode reward is 26.18. {'loss': 0.0275, 'episode_len': 28.0}\n(19.0s,+1.1s)\tIteration 410, current mean episode reward is 34.1. {'loss': 0.0507, 'episode_len': 10.0}\n(20.4s,+1.4s)\tIteration 420, current mean episode reward is 30.28. {'loss': 0.044, 'episode_len': 28.0}\n1013 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 5039\n(21.1s,+0.8s)\tIteration 430, current mean episode reward is 13.38. {'loss': 0.0864, 'episode_len': 9.0}\n(22.4s,+1.2s)\tIteration 440, current mean episode reward is 29.44. {'loss': 0.1035, 'episode_len': 31.0}\n(23.5s,+1.2s)\tIteration 450, current mean episode reward is 27.68. {'loss': 0.0687, 'episode_len': 10.0}\n(24.9s,+1.3s)\tIteration 460, current mean episode reward is 29.5. {'loss': 0.0421, 'episode_len': 31.0}\n1008 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 6047\n(26.1s,+1.3s)\tIteration 470, current mean episode reward is 26.86. {'loss': 0.1173, 'episode_len': 9.0}\n(28.1s,+1.9s)\tIteration 480, current mean episode reward is 36.38. {'loss': 0.0843, 'episode_len': 25.0}\n(29.8s,+1.7s)\tIteration 490, current mean episode reward is 39.16. {'loss': 0.0528, 'episode_len': 40.0}\n1012 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 7059\n(33.1s,+3.3s)\tIteration 500, current mean episode reward is 102.68. {'loss': 0.0677, 'episode_len': 87.0}\n(35.5s,+2.4s)\tIteration 510, current mean episode reward is 40.3. {'loss': 0.0723, 'episode_len': 131.0}\n1069 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 8128\n(40.2s,+4.7s)\tIteration 520, current mean episode reward is 168.66. {'loss': 0.0665, 'episode_len': 49.0}\n1066 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 9194\n(45.3s,+5.2s)\tIteration 530, current mean episode reward is 180.02. {'loss': 0.2601, 'episode_len': 88.0}\n1180 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 10374\n(49.9s,+4.6s)\tIteration 540, current mean episode reward is 76.84. {'loss': 0.219, 'episode_len': 159.0}\n1092 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 11466\n(55.5s,+5.6s)\tIteration 550, current mean episode reward is 52.26. {'loss': 0.2305, 'episode_len': 23.0}\n1013 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 12479\n(60.8s,+5.3s)\tIteration 560, current mean episode reward is 130.96. {'loss': 0.5407, 'episode_len': 125.0}\n1126 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 13605\n(65.1s,+4.3s)\tIteration 570, current mean episode reward is 54.3. {'loss': 0.5089, 'episode_len': 59.0}\n1095 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 14700\n(69.2s,+4.2s)\tIteration 580, current mean episode reward is 21.9. {'loss': 0.9219, 'episode_len': 142.0}\n1001 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 15701\n(75.4s,+6.2s)\tIteration 590, current mean episode reward is 188.68. {'loss': 0.5377, 'episode_len': 110.0}\n1057 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 16758\n(79.2s,+3.8s)\tIteration 600, current mean episode reward is 73.12. {'loss': 1.071, 'episode_len': 69.0}\n(83.2s,+4.0s)\tIteration 610, current mean episode reward is 19.0. {'loss': 0.9702, 'episode_len': 161.0}\n1066 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 17824\n1003 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 18827\n(88.4s,+5.2s)\tIteration 620, current mean episode reward is 93.46. {'loss': 0.7306, 'episode_len': 58.0}\n(91.6s,+3.3s)\tIteration 630, current mean episode reward is 23.06. {'loss': 1.1861, 'episode_len': 36.0}\n1015 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 19842\n(96.3s,+4.7s)\tIteration 640, current mean episode reward is 123.36. {'loss': 1.5334, 'episode_len': 63.0}\n1134 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 20976\n(100.0s,+3.7s)\tIteration 650, current mean episode reward is 18.26. {'loss': 0.4355, 'episode_len': 37.0}\n1011 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 21987\n(103.8s,+3.8s)\tIteration 660, current mean episode reward is 97.28. {'loss': 1.5478, 'episode_len': 70.0}\n1051 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 23038\n(108.1s,+4.3s)\tIteration 670, current mean episode reward is 37.06. {'loss': 1.9885, 'episode_len': 43.0}\n1024 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 24062\n(111.3s,+3.2s)\tIteration 680, current mean episode reward is 82.94. {'loss': 1.8322, 'episode_len': 62.0}\n1078 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 25140\n(117.1s,+5.9s)\tIteration 690, current mean episode reward is 149.74. {'loss': 2.5579, 'episode_len': 190.0}\n(121.1s,+4.0s)\tIteration 700, current mean episode reward is 42.2. {'loss': 1.9797, 'episode_len': 83.0}\n1120 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 26260\n1035 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 27295\n(125.3s,+4.2s)\tIteration 710, current mean episode reward is 21.78. {'loss': 2.5278, 'episode_len': 191.0}\n1033 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 28328\n(131.8s,+6.4s)\tIteration 720, current mean episode reward is 193.18. {'loss': 3.6603, 'episode_len': 69.0}\n1066 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 29394\n(135.5s,+3.8s)\tIteration 730, current mean episode reward is 33.24. {'loss': 2.1838, 'episode_len': 118.0}\n(138.6s,+3.1s)\tIteration 740, current mean episode reward is 21.22. {'loss': 2.5438, 'episode_len': 76.0}\n1046 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 30440\n(142.4s,+3.8s)\tIteration 750, current mean episode reward is 92.52. {'loss': 3.912, 'episode_len': 137.0}\n1017 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 31457\n(146.3s,+3.9s)\tIteration 760, current mean episode reward is 15.24. {'loss': 4.8485, 'episode_len': 135.0}\n1048 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 32505\n(148.7s,+2.4s)\tIteration 770, current mean episode reward is 17.34. {'loss': 4.7454, 'episode_len': 106.0}\n(150.5s,+1.8s)\tIteration 780, current mean episode reward is 15.42. {'loss': 6.1417, 'episode_len': 29.0}\n(152.7s,+2.2s)\tIteration 790, current mean episode reward is 75.56. {'loss': 2.6205, 'episode_len': 42.0}\n1053 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 33558\n(157.2s,+4.5s)\tIteration 800, current mean episode reward is 199.22. {'loss': 2.111, 'episode_len': 26.0}\nIn 800 iteration, current mean episode reward 199.220 is greater than reward threshold 195.0. Congratulation! Now we exit the training process.\n"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pytorch_trainer, pytorch_stat = run(DQNTrainer, dict(\n",
    "    max_iteration=2000,\n",
    "    evaluate_interval=10, \n",
    "    learning_rate=0.01,\n",
    "    clip_norm=10.0,\n",
    "    memory_size=50000,\n",
    "    learn_start=1000,\n",
    "    eps=0.1,\n",
    "    target_update_freq=1000,\n",
    "    batch_size=32,\n",
    "    env_name=\"CartPole-v0\",\n",
    "), reward_threshold=195.0)\n",
    "\n",
    "reward = pytorch_trainer.evaluate()\n",
    "assert reward > 195.0, \"Check your codes. \" \\\n",
    "    \"Your agent should achieve {} reward in 1000 iterations.\" \\\n",
    "    \"But it achieve {} reward in evaluation.\".format(195.0, reward)\n",
    "\n",
    "# Should solve the task in 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Average episode reward for your Pytorch agent in CartPole-v0:  200.0\n"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# You should see a pop up window which display the movement of the cart and pole.\n",
    "print(\"Average episode reward for your Pytorch agent in CartPole-v0: \",\n",
    "      pytorch_trainer.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(34.1s,+34.1s)\tIteration 0, current mean episode reward is -21.0. {'loss': nan, 'episode_len': 1022.0}\n10194 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 10194\n(94.6s,+60.5s)\tIteration 10, current mean episode reward is -21.0. {'loss': 1.9354, 'episode_len': 1184.0}\n11027 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 21221\n(167.8s,+73.2s)\tIteration 20, current mean episode reward is -21.0. {'loss': 0.7411, 'episode_len': 1029.0}\n11082 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 32303\n(239.5s,+71.7s)\tIteration 30, current mean episode reward is -20.7. {'loss': 0.5259, 'episode_len': 1056.0}\n10016 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 42319\n(310.2s,+70.8s)\tIteration 40, current mean episode reward is -21.0. {'loss': 0.3734, 'episode_len': 1160.0}\n10947 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 53266\n(386.9s,+76.7s)\tIteration 50, current mean episode reward is -20.5. {'loss': 0.3149, 'episode_len': 1015.0}\n10498 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 63764\n(463.0s,+76.1s)\tIteration 60, current mean episode reward is -21.0. {'loss': 0.3174, 'episode_len': 1480.0}\n10132 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 73896\n(543.3s,+80.3s)\tIteration 70, current mean episode reward is -21.0. {'loss': 0.2296, 'episode_len': 1178.0}\n10134 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 84030\n(621.6s,+78.3s)\tIteration 80, current mean episode reward is -21.0. {'loss': 0.2251, 'episode_len': 1118.0}\n10868 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 94898\n(694.0s,+72.4s)\tIteration 90, current mean episode reward is -21.0. {'loss': 0.2291, 'episode_len': 1027.0}\n10987 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 105885\n(772.7s,+78.6s)\tIteration 100, current mean episode reward is -21.0. {'loss': 0.2319, 'episode_len': 1555.0}\n10671 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 116556\n(860.5s,+87.9s)\tIteration 110, current mean episode reward is -20.4. {'loss': 0.1218, 'episode_len': 1099.0}\n10767 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 127323\n10275 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 137598\n(936.9s,+76.4s)\tIteration 120, current mean episode reward is -21.0. {'loss': 0.0926, 'episode_len': 1177.0}\n10157 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 147755\n(1016.7s,+79.8s)\tIteration 130, current mean episode reward is -21.0. {'loss': 0.2021, 'episode_len': 1280.0}\n10573 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 158328\n(1095.7s,+78.9s)\tIteration 140, current mean episode reward is -21.0. {'loss': 0.2146, 'episode_len': 1320.0}\n10339 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 168667\n(1172.9s,+77.2s)\tIteration 150, current mean episode reward is -21.0. {'loss': 0.1229, 'episode_len': 1014.0}\n10003 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 178670\n(1248.7s,+75.8s)\tIteration 160, current mean episode reward is -21.0. {'loss': 0.1483, 'episode_len': 1311.0}\n10355 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 189025\n(1331.8s,+83.2s)\tIteration 170, current mean episode reward is -20.9. {'loss': 0.2738, 'episode_len': 1319.0}\n10411 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 199436\n(1408.3s,+76.4s)\tIteration 180, current mean episode reward is -21.0. {'loss': 0.204, 'episode_len': 1198.0}\n10249 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 209685\n(1487.2s,+79.0s)\tIteration 190, current mean episode reward is -20.9. {'loss': 0.146, 'episode_len': 1203.0}\n10887 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 220572\n(1560.2s,+73.0s)\tIteration 200, current mean episode reward is -21.0. {'loss': 0.1662, 'episode_len': 1392.0}\n10069 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 230641\n10074 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 240715\n(1638.2s,+78.0s)\tIteration 210, current mean episode reward is -21.0. {'loss': 0.2154, 'episode_len': 1202.0}\n10691 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 251406\n(1718.7s,+80.4s)\tIteration 220, current mean episode reward is -20.8. {'loss': 0.0805, 'episode_len': 1159.0}\n10277 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 261683\n(1795.4s,+76.7s)\tIteration 230, current mean episode reward is -21.0. {'loss': 0.1737, 'episode_len': 1171.0}\n10358 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 272041\n(1872.3s,+76.9s)\tIteration 240, current mean episode reward is -20.8. {'loss': 0.1836, 'episode_len': 1113.0}\n10492 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 282533\n(1950.9s,+78.6s)\tIteration 250, current mean episode reward is -21.0. {'loss': 0.1423, 'episode_len': 1186.0}\n10247 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 292780\n(2027.0s,+76.1s)\tIteration 260, current mean episode reward is -21.0. {'loss': 0.0803, 'episode_len': 1298.0}\n10700 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 303480\n(2104.6s,+77.7s)\tIteration 270, current mean episode reward is -21.0. {'loss': 0.0965, 'episode_len': 1098.0}\n11122 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 314602\n(2181.3s,+76.6s)\tIteration 280, current mean episode reward is -21.0. {'loss': 0.1953, 'episode_len': 1292.0}\n10215 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 324817\n(2263.9s,+82.6s)\tIteration 290, current mean episode reward is -21.0. {'loss': 0.1357, 'episode_len': 1204.0}\n10406 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 335223\n(2342.1s,+78.2s)\tIteration 300, current mean episode reward is -21.0. {'loss': 0.129, 'episode_len': 1206.0}\n10150 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 345373\n(2419.8s,+77.7s)\tIteration 310, current mean episode reward is -20.7. {'loss': 0.0956, 'episode_len': 1023.0}\n10883 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 356256\n10221 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 366477\n(2492.9s,+73.0s)\tIteration 320, current mean episode reward is -20.9. {'loss': 0.1027, 'episode_len': 1112.0}\n10213 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 376690\n(2569.6s,+76.8s)\tIteration 330, current mean episode reward is -21.0. {'loss': 0.088, 'episode_len': 1133.0}\n10517 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 387207\n(2656.8s,+87.2s)\tIteration 340, current mean episode reward is -20.3. {'loss': 0.0825, 'episode_len': 1095.0}\n10443 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 397650\n(2735.3s,+78.5s)\tIteration 350, current mean episode reward is -21.0. {'loss': 0.0444, 'episode_len': 1047.0}\n10226 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 407876\n(2814.5s,+79.2s)\tIteration 360, current mean episode reward is -21.0. {'loss': 0.1243, 'episode_len': 1195.0}\n10279 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 418155\n(2893.4s,+78.9s)\tIteration 370, current mean episode reward is -21.0. {'loss': 0.0943, 'episode_len': 1123.0}\n10672 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 428827\n(2971.4s,+78.0s)\tIteration 380, current mean episode reward is -20.9. {'loss': 0.0545, 'episode_len': 1186.0}\n10111 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 438938\n(3054.2s,+82.8s)\tIteration 390, current mean episode reward is -20.8. {'loss': 0.0842, 'episode_len': 1182.0}\n10310 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 449248\n(3140.1s,+85.9s)\tIteration 400, current mean episode reward is -21.0. {'loss': 0.0815, 'episode_len': 1158.0}\n10474 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 459722\n10328 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 470050\n(3219.7s,+79.6s)\tIteration 410, current mean episode reward is -21.0. {'loss': 0.1014, 'episode_len': 1044.0}\n10046 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 480096\n(3296.5s,+76.8s)\tIteration 420, current mean episode reward is -20.6. {'loss': 0.1216, 'episode_len': 1308.0}\n10357 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 490453\n(3370.0s,+73.5s)\tIteration 430, current mean episode reward is -21.0. {'loss': 0.1189, 'episode_len': 1110.0}\n10088 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 500541\n(3450.5s,+80.5s)\tIteration 440, current mean episode reward is -21.0. {'loss': 0.0549, 'episode_len': 1084.0}\n10614 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 511155\n(3536.1s,+85.6s)\tIteration 450, current mean episode reward is -21.0. {'loss': 0.1272, 'episode_len': 1010.0}\n10952 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 522107\n(3614.3s,+78.2s)\tIteration 460, current mean episode reward is -21.0. {'loss': 0.0861, 'episode_len': 1061.0}\n10272 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 532379\n(3693.3s,+79.0s)\tIteration 470, current mean episode reward is -21.0. {'loss': 0.0706, 'episode_len': 1057.0}\n10878 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 543257\n(3767.7s,+74.4s)\tIteration 480, current mean episode reward is -21.0. {'loss': 0.0854, 'episode_len': 1256.0}\n10595 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 553852\n(3843.4s,+75.7s)\tIteration 490, current mean episode reward is -21.0. {'loss': 0.0735, 'episode_len': 1100.0}\n10726 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 564578\n(3924.0s,+80.6s)\tIteration 500, current mean episode reward is -21.0. {'loss': 0.0985, 'episode_len': 1093.0}\n"
    }
   ],
   "source": [
    "# [optional] BONUS!!! Train DQN in \"Pong-ram-v0\" environment\n",
    "# Tune the hyperparameter and take some time to train agent\n",
    "# You need to install gym[atari] first via `pip install gym[atari]`\n",
    "\n",
    "pytorch_trainer2, _ = run(DQNTrainer, dict(\n",
    "    max_episode_length=10000,\n",
    "    max_iteration=500,\n",
    "    evaluate_interval=10,\n",
    "    evaluate_num_episodes=10,\n",
    "    learning_rate=0.0001,\n",
    "    clip_norm=10.0,\n",
    "    memory_size=1000000,\n",
    "    learn_start=10000,\n",
    "    eps=0.02,\n",
    "    target_update_freq=10000,\n",
    "    learn_freq=4,\n",
    "    batch_size=32,\n",
    "    env_name=\"Pong-ram-v0\"\n",
    "), reward_threshold=-20.0)\n",
    "\n",
    "# This environment is hard to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [optional] If you have train the agent in Pont-ram-v0, please save the weights so that\n",
    "# we can restore it. Please include the pong-agent.pkl into the zip.\n",
    "\n",
    "import pickle\n",
    "with open(\"pong-agent.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pytorch_trainer2.network.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8fd42f885f4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m print(\"Average episode reward for your Pytorch agent in Pong-ram-v0: \",\n\u001b[1;32m----> 2\u001b[1;33m       pytorch_trainer2.evaluate(1, render=True))\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-b78184d9e09e>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, num_episodes, *args, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m             self.process_state(raw_state), eps=0.0)\n\u001b[0;32m    105\u001b[0m         result = evaluate(policy, num_episodes, seed=self.seed,\n\u001b[1;32m--> 106\u001b[1;33m                           env_name=self.env_name, *args, **kwargs)\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-fa6663dcec7c>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(policy, num_episodes, seed, env_name, render)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\VScode_Workspace\\Python\\IRL_Exercise\\assignments\\assignment2\\utils.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(sleep)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Average episode reward for your Pytorch agent in Pong-ram-v0: \",\n",
    "      pytorch_trainer2.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Conclusion and Discussion\n",
    "\n",
    "In this assignment, we learn how to build several function approximation algorithm, how to implement basic gradient descent methods and how to use pytorch.\n",
    "\n",
    "It's OK to leave the following cells empty. In the next markdown cell, you can write whatever you like. Like the suggestion on the course, the confusing problems in the assignments, and so on.\n",
    "\n",
    "If you want to do more investigation, feel free to open new cells via `Esc + B` after the next cells and write codes in it, so that you can reuse some result in this notebook. Remember to write sufficient comments and documents to let others know what you are doing.\n",
    "\n",
    "Following the submission instruction in the assignment to submit your assignment to our staff. Thank you!\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('introRL': conda)",
   "language": "python",
   "name": "python37764bitintrorlconda3256167ded224a87aeb57a79ea36bd51"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
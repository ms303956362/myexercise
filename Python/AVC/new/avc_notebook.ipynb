{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2wiulCsyjJS",
        "outputId": "01b4e6d8-0927-4b6e-d5f9-38d6ce6d661a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch as t\n",
        "import torch.nn as nn\n",
        "from machin.frame.algorithms import MADDPG\n",
        "from machin.utils.logging import default_logger as logger\n",
        "\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "# from powerenv import PowerNet\n",
        "\n",
        "\n",
        "class VoltageController:\n",
        "    def __init__(self, ppc, agentBuses, agentGens, Actor, Critic) -> None:\n",
        "        self.ppc = ppc\n",
        "        self.agentBuses = agentBuses\n",
        "        self.agentGens = agentGens\n",
        "        self.powerenv = PowerNet(ppc, agentBuses, agentGens)\n",
        "        self.agent_num = len(agentGens)\n",
        "        self.states_dim = list(map(lambda x : 4 * x, list(map(len, agentBuses))))\n",
        "        self.actions_dim = list(map(len, agentGens))\n",
        "        self.maddpg = MADDPG(\n",
        "                        [Actor(sdim, adim) for sdim, adim in zip(self.states_dim, self.actions_dim)],\n",
        "                        [Actor(sdim, adim) for sdim, adim in zip(self.states_dim, self.actions_dim)],\n",
        "                        [Critic(sum(self.states_dim), sum(self.actions_dim)) for _ in range(self.agent_num)],\n",
        "                        [Critic(sum(self.states_dim), sum(self.actions_dim)) for _ in range(self.agent_num)],\n",
        "                        [list(range(self.agent_num))] * self.agent_num,\n",
        "                        t.optim.Adam,\n",
        "                        nn.MSELoss(reduction='sum')\n",
        "                    )\n",
        "    \n",
        "    def train(self, max_episodes=100, max_steps=20, solved_reward = 0.1, solved_repeat = 5):\n",
        "        # training\n",
        "        episode, step, reward_fulfilled = 0, 0, 0\n",
        "        smoothed_total_reward = 0\n",
        "\n",
        "        action_losses = [[] for _ in range(len(self.agentBuses))]\n",
        "        value_losses = [[] for _ in range(len(self.agentGens))]\n",
        "        time_step = [0]\n",
        "        while episode < max_episodes:\n",
        "            episode += 1\n",
        "            total_reward = 0\n",
        "            terminal = False\n",
        "            step = 0\n",
        "            states = [t.tensor(st, dtype=t.float32).view(1, st.shape[0])\n",
        "                    for st in self.powerenv.reset()]\n",
        "            \n",
        "            while not terminal and step <= max_steps:\n",
        "                step += 1\n",
        "                with t.no_grad():\n",
        "                    old_states = states\n",
        "                    # agent model inference\n",
        "                    result = self.maddpg.act([{\"state\": st} for st in states])\n",
        "                    action = [act.numpy() for act in result]\n",
        "                    \n",
        "                    states, rewards, terminals, _ = self.powerenv.step(action)\n",
        "                    states = [t.tensor(st, dtype=t.float32) for st in states]\n",
        "                    total_reward += float(sum(rewards)) / self.agent_num\n",
        "                    terminal = bool(np.prod(terminals))\n",
        "\n",
        "                    self.maddpg.store_transitions([{\n",
        "                        \"state\": {\"state\": ost},\n",
        "                        \"action\": {\"action\": act},\n",
        "                        \"next_state\": {\"state\": st},\n",
        "                        \"reward\": float(rew),\n",
        "                        \"terminal\": term or step == max_steps\n",
        "                    } for ost, act, st, rew, term in zip(\n",
        "                        old_states, result, states, rewards, terminals\n",
        "                    )])\n",
        "            \n",
        "            # total reward is divided by steps here, since:\n",
        "            # \"Agents are rewarded based on minimum agent distance\n",
        "            #  to each landmark, penalized for collisions\"\n",
        "            total_reward /= step\n",
        "\n",
        "            # update, update more if episode is longer, else less\n",
        "            if episode > int(0.2 * max_episodes):\n",
        "                for _ in range(step):\n",
        "                    self.maddpg.update()\n",
        "\n",
        "                    # if you want to plot loss, please revise the `update` function, return `all_loss` directly and uncomment the code in line 81~85, line 174~185\n",
        "                    # loss = self.maddpg.update()\n",
        "                    # time_step.append(time_step[-1] + 1)\n",
        "                    # for all, vll, (al, vl) in zip(action_losses, value_losses, loss):\n",
        "                    #     all.append(al)\n",
        "                    #     vll.append(vl)\n",
        "\n",
        "            # show reward\n",
        "            smoothed_total_reward = (smoothed_total_reward * 0.9 +\n",
        "                                    total_reward * 0.1)\n",
        "            logger.info(\"Episode {} total reward={:.2f}\"\n",
        "                        .format(episode, smoothed_total_reward))\n",
        "\n",
        "            if smoothed_total_reward > solved_reward and episode > int(0.3 * max_episodes):\n",
        "                reward_fulfilled += 1\n",
        "                if reward_fulfilled >= solved_repeat:\n",
        "                    logger.info(\"Environment solved!\")\n",
        "                    exit(0)\n",
        "            else:\n",
        "                reward_fulfilled = 0\n",
        "        time_step.pop()\n",
        "        return action_losses, value_losses, time_step\n",
        "\n",
        "    def act(self, ppc):\n",
        "        with t.no_grad():\n",
        "            sts = self.powerenv.get_state(ppc)\n",
        "            states = [t.tensor(st, dtype=t.float32).view(1, st.shape[0])\n",
        "                    for st in sts]\n",
        "            result = self.maddpg.act(states)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from pypower.api import case39\n",
        "    import matplotlib.pyplot as plt\n",
        "    # model definition\n",
        "    class Actor(nn.Module):\n",
        "        def __init__(self, state_dim, action_dim):\n",
        "            super(Actor, self).__init__()\n",
        "\n",
        "            self.fc1 = nn.Linear(state_dim, 16)\n",
        "            self.fc2 = nn.Linear(16, 16)\n",
        "            self.fc3 = nn.Linear(16, action_dim)\n",
        "\n",
        "        def forward(self, state):\n",
        "            a = t.relu(self.fc1(state))\n",
        "            a = t.relu(self.fc2(a))\n",
        "            a = t.tanh(self.fc3(a))\n",
        "            a = 1 + 0.1 * a\n",
        "            return a\n",
        "\n",
        "\n",
        "    class Critic(nn.Module):\n",
        "        def __init__(self, state_dim, action_dim):\n",
        "            # This critic implementation is shared by the prey(DDPG) and\n",
        "            # predators(MADDPG)\n",
        "            # Note: For MADDPG\n",
        "            #       state_dim is the dimension of all states from all agents.\n",
        "            #       action_dim is the dimension of all actions from all agents.\n",
        "            super(Critic, self).__init__()\n",
        "\n",
        "            self.fc1 = nn.Linear(state_dim + action_dim, 16)\n",
        "            self.fc2 = nn.Linear(16, 16)\n",
        "            self.fc3 = nn.Linear(16, 1)\n",
        "\n",
        "        def forward(self, state, action):\n",
        "            state_action = t.cat([state, action], 1)\n",
        "            q = t.relu(self.fc1(state_action))\n",
        "            q = t.relu(self.fc2(q))\n",
        "            q = self.fc3(q)\n",
        "            return q\n",
        "\n",
        "    # powernet environement configuration\n",
        "    ppc = case39()\n",
        "\n",
        "    # the number of buses controled by each agent\n",
        "    agentBuses = [\n",
        "        [2, 14, 15, 16, 17, 18, 25, 26, 27, 28, 29, 30, 37, 38],\n",
        "        [13, 19, 20, 21, 22, 23, 24, 33, 34, 35, 36],\n",
        "        [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 31, 32, 39]\n",
        "    ]\n",
        "    agentBuses = [[b - 1 for b in buses] for buses in agentBuses]\n",
        "\n",
        "    # the number of generators controled by each agent\n",
        "    agentGens = [\n",
        "        [0, 7, 8],\n",
        "        [3, 4, 5, 6],\n",
        "        [1, 2, 9]\n",
        "    ]\n",
        "\n",
        "    # voltage controller\n",
        "    vc = VoltageController(ppc, agentBuses, agentGens, Actor, Critic)\n",
        "    vc.train()\n",
        "\n",
        "    # if you want to plot loss, please revise the `update` function, return `all_loss` directly and uncomment the code in line 81~85, line 174~185\n",
        "    # action_losses, value_losses, time_step = vc.train()\n",
        "    # num = 1\n",
        "    # for al in action_losses:\n",
        "    #     plt.subplot(2, 3, num)\n",
        "    #     plt.plot(time_step, al)\n",
        "    #     num += 1\n",
        "    # for vl in value_losses:\n",
        "    #     plt.subplot(2, 3, num)\n",
        "    #     plt.plot(time_step, vl)\n",
        "    #     num += 1\n",
        "    # plt.savefig(str(num))\n",
        "    # plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m[2020-10-26 02:44:10,692] <WARNING>:default_logger:You have not specified the i/o device ofyour model <class 'torch.jit.RecursiveScriptModule'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by thisoperation.\u001b[0m\n",
            "\u001b[33m[2020-10-26 02:44:10,700] <WARNING>:default_logger:You have not specified the i/o device ofyour model <class 'torch.jit.RecursiveScriptModule'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by thisoperation.\u001b[0m\n",
            "\u001b[33m[2020-10-26 02:44:10,707] <WARNING>:default_logger:You have not specified the i/o device ofyour model <class 'torch.jit.RecursiveScriptModule'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by thisoperation.\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:11,313] <INFO>:default_logger:Episode 1 total reward=-0.05\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:11,903] <INFO>:default_logger:Episode 2 total reward=-0.09\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:12,513] <INFO>:default_logger:Episode 3 total reward=-0.14\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:13,136] <INFO>:default_logger:Episode 4 total reward=-0.17\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:13,742] <INFO>:default_logger:Episode 5 total reward=-0.19\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:14,358] <INFO>:default_logger:Episode 6 total reward=-0.23\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:14,950] <INFO>:default_logger:Episode 7 total reward=-0.25\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:15,557] <INFO>:default_logger:Episode 8 total reward=-0.27\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:16,166] <INFO>:default_logger:Episode 9 total reward=-0.28\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:16,759] <INFO>:default_logger:Episode 10 total reward=-0.30\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:17,362] <INFO>:default_logger:Episode 11 total reward=-0.31\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:17,995] <INFO>:default_logger:Episode 12 total reward=-0.33\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:18,667] <INFO>:default_logger:Episode 13 total reward=-0.34\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:19,281] <INFO>:default_logger:Episode 14 total reward=-0.35\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:19,882] <INFO>:default_logger:Episode 15 total reward=-0.37\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:20,473] <INFO>:default_logger:Episode 16 total reward=-0.38\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:21,059] <INFO>:default_logger:Episode 17 total reward=-0.40\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:21,692] <INFO>:default_logger:Episode 18 total reward=-0.40\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:22,289] <INFO>:default_logger:Episode 19 total reward=-0.40\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:22,884] <INFO>:default_logger:Episode 20 total reward=-0.40\u001b[0m\n",
            "\u001b[33m[2020-10-26 02:44:23,576] <WARNING>:default_logger:You have not specified the i/o device of your model <class '__main__.Actor'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
            "\u001b[33m[2020-10-26 02:44:23,578] <WARNING>:default_logger:You have not specified the i/o device of your model <class '__main__.Actor'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
            "\u001b[33m[2020-10-26 02:44:23,588] <WARNING>:default_logger:You have not specified the i/o device of your model <class '__main__.Critic'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
            "\u001b[33m[2020-10-26 02:44:23,591] <WARNING>:default_logger:You have not specified the i/o device of your model <class '__main__.Critic'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
            "\u001b[33m[2020-10-26 02:44:23,601] <WARNING>:default_logger:You have not specified the i/o device of your model <class '__main__.Critic'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
            "\u001b[33m[2020-10-26 02:44:23,602] <WARNING>:default_logger:You have not specified the i/o device of your model <class '__main__.Critic'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
            "\u001b[33m[2020-10-26 02:44:23,745] <WARNING>:default_logger:You have not specified the i/o device of your model <class '__main__.Actor'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
            "\u001b[33m[2020-10-26 02:44:23,749] <WARNING>:default_logger:You have not specified the i/o device of your model <class '__main__.Actor'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
            "\u001b[33m[2020-10-26 02:44:23,785] <WARNING>:default_logger:You have not specified the i/o device of your model <class '__main__.Actor'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
            "\u001b[33m[2020-10-26 02:44:23,792] <WARNING>:default_logger:You have not specified the i/o device of your model <class '__main__.Critic'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
            "\u001b[33m[2020-10-26 02:44:23,794] <WARNING>:default_logger:You have not specified the i/o device of your model <class '__main__.Critic'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
            "\u001b[33m[2020-10-26 02:44:23,798] <WARNING>:default_logger:You have not specified the i/o device of your model <class '__main__.Actor'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by this operation.\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:24,330] <INFO>:default_logger:Episode 21 total reward=-0.41\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:25,492] <INFO>:default_logger:Episode 22 total reward=-0.38\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:26,699] <INFO>:default_logger:Episode 23 total reward=-0.37\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:27,889] <INFO>:default_logger:Episode 24 total reward=-0.36\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:29,030] <INFO>:default_logger:Episode 25 total reward=-0.36\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:30,231] <INFO>:default_logger:Episode 26 total reward=-0.35\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:31,356] <INFO>:default_logger:Episode 27 total reward=-0.35\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:32,488] <INFO>:default_logger:Episode 28 total reward=-0.35\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:33,673] <INFO>:default_logger:Episode 29 total reward=-0.34\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:34,796] <INFO>:default_logger:Episode 30 total reward=-0.34\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:35,943] <INFO>:default_logger:Episode 31 total reward=-0.34\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:37,119] <INFO>:default_logger:Episode 32 total reward=-0.33\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:38,233] <INFO>:default_logger:Episode 33 total reward=-0.33\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:39,411] <INFO>:default_logger:Episode 34 total reward=-0.33\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:40,562] <INFO>:default_logger:Episode 35 total reward=-0.33\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:41,705] <INFO>:default_logger:Episode 36 total reward=-0.33\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:42,920] <INFO>:default_logger:Episode 37 total reward=-0.33\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:44,055] <INFO>:default_logger:Episode 38 total reward=-0.32\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:45,211] <INFO>:default_logger:Episode 39 total reward=-0.33\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:46,345] <INFO>:default_logger:Episode 40 total reward=-0.35\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:47,510] <INFO>:default_logger:Episode 41 total reward=-0.36\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:48,706] <INFO>:default_logger:Episode 42 total reward=-0.37\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:49,866] <INFO>:default_logger:Episode 43 total reward=-0.37\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:51,018] <INFO>:default_logger:Episode 44 total reward=-0.38\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:52,154] <INFO>:default_logger:Episode 45 total reward=-0.38\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:53,281] <INFO>:default_logger:Episode 46 total reward=-0.39\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:54,419] <INFO>:default_logger:Episode 47 total reward=-0.39\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:55,596] <INFO>:default_logger:Episode 48 total reward=-0.40\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:56,791] <INFO>:default_logger:Episode 49 total reward=-0.41\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:58,001] <INFO>:default_logger:Episode 50 total reward=-0.41\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:44:59,175] <INFO>:default_logger:Episode 51 total reward=-0.42\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:00,308] <INFO>:default_logger:Episode 52 total reward=-0.43\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:01,583] <INFO>:default_logger:Episode 53 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:02,750] <INFO>:default_logger:Episode 54 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:03,961] <INFO>:default_logger:Episode 55 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:05,165] <INFO>:default_logger:Episode 56 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:06,361] <INFO>:default_logger:Episode 57 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:07,565] <INFO>:default_logger:Episode 58 total reward=-0.45\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:08,753] <INFO>:default_logger:Episode 59 total reward=-0.45\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:09,960] <INFO>:default_logger:Episode 60 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:11,149] <INFO>:default_logger:Episode 61 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:12,287] <INFO>:default_logger:Episode 62 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:13,462] <INFO>:default_logger:Episode 63 total reward=-0.43\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:14,644] <INFO>:default_logger:Episode 64 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:15,866] <INFO>:default_logger:Episode 65 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:17,107] <INFO>:default_logger:Episode 66 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:18,329] <INFO>:default_logger:Episode 67 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:19,486] <INFO>:default_logger:Episode 68 total reward=-0.45\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:20,654] <INFO>:default_logger:Episode 69 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:21,829] <INFO>:default_logger:Episode 70 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:22,968] <INFO>:default_logger:Episode 71 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:24,110] <INFO>:default_logger:Episode 72 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:25,271] <INFO>:default_logger:Episode 73 total reward=-0.45\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:26,442] <INFO>:default_logger:Episode 74 total reward=-0.45\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:27,580] <INFO>:default_logger:Episode 75 total reward=-0.45\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:28,731] <INFO>:default_logger:Episode 76 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:29,893] <INFO>:default_logger:Episode 77 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:31,048] <INFO>:default_logger:Episode 78 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:32,232] <INFO>:default_logger:Episode 79 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:33,407] <INFO>:default_logger:Episode 80 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:34,578] <INFO>:default_logger:Episode 81 total reward=-0.45\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:35,732] <INFO>:default_logger:Episode 82 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:36,905] <INFO>:default_logger:Episode 83 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:38,059] <INFO>:default_logger:Episode 84 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:39,270] <INFO>:default_logger:Episode 85 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:40,452] <INFO>:default_logger:Episode 86 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:41,634] <INFO>:default_logger:Episode 87 total reward=-0.43\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:42,832] <INFO>:default_logger:Episode 88 total reward=-0.43\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:44,179] <INFO>:default_logger:Episode 89 total reward=-0.43\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:45,459] <INFO>:default_logger:Episode 90 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:46,736] <INFO>:default_logger:Episode 91 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:47,971] <INFO>:default_logger:Episode 92 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:49,239] <INFO>:default_logger:Episode 93 total reward=-0.44\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:50,514] <INFO>:default_logger:Episode 94 total reward=-0.45\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:51,792] <INFO>:default_logger:Episode 95 total reward=-0.45\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:53,100] <INFO>:default_logger:Episode 96 total reward=-0.45\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:54,286] <INFO>:default_logger:Episode 97 total reward=-0.45\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:55,421] <INFO>:default_logger:Episode 98 total reward=-0.45\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:56,590] <INFO>:default_logger:Episode 99 total reward=-0.45\u001b[0m\n",
            "\u001b[32m[2020-10-26 02:45:57,749] <INFO>:default_logger:Episode 100 total reward=-0.45\u001b[0m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QqltDGev_1w",
        "outputId": "40b51001-8546-4de3-8b5d-407526d59b5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "# ! pip install machin\n",
        "from pypower.api import runpf, ppoption\n",
        "from pypower.idx_bus import PD, QD, VM, VA\n",
        "from pypower.idx_gen import GEN_BUS, PG, QG, VG\n",
        "\n",
        "class PowerNet(gym.Env):\n",
        "    def __init__(self, ppc, areaBuses, areaGens, vref = 1.0, alpha=0.1, beta=0.5) -> None:\n",
        "        \"\"\"\n",
        "        Power net environment for reinforcement learning, implemented by using pypower, gym\n",
        "        @ppc: dict, pypower case\n",
        "        @areaBuses: List[List[]], i-th term is the list of bus numbers in i-th area\n",
        "        @areaGens: List[List[]], i-th term is the list of generator numbers in i-th area\n",
        "        @vref: reference voltage of the system, p.u.\n",
        "        \"\"\"\n",
        "        self.ppc = ppc\n",
        "        self.ppopt = ppoption(PF_ALG=1, VERBOSE=0, OUT_ALL=0)\n",
        "        self.ppc0 = deepcopy(self.ppc)\n",
        "        self.areaBuses = areaBuses\n",
        "        self.areaGens = areaGens\n",
        "        self.vref = vref\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.state_dim = list(map(lambda x: 4 * x, list(map(len, areaBuses))))  # each term in states list is PD, QD, V, theta of buses in each area\n",
        "        self.action_dim = list(map(len, areaGens)) # each term in actions list is voltage of generators in each area\n",
        "        assert(sum(self.state_dim) / 4 == len(ppc['bus']))\n",
        "        assert(sum(self.action_dim) == len(ppc['gen']))\n",
        "    \n",
        "    def reset(self):\n",
        "        self.ppc = deepcopy(self.ppc0)\n",
        "        # random load level, 0.7~1.3\n",
        "        level = 1 + 0.3 * np.random.rand()\n",
        "        self.ppc['bus'][:, (PD, QD)] *= level\n",
        "        self.ppc['gen'][:, (PG, QG)] *= level\n",
        "        state = [np.zeros(dim) for dim in self.state_dim]\n",
        "        state_bus = self.ppc['bus'][:, (PD, QD, VM, VA)]\n",
        "        state_gen = self.ppc['gen'][:, (GEN_BUS, PG, QG)]\n",
        "        state_bus = self._cal_inject_power(state_bus, state_gen)\n",
        "        for j, buses in enumerate(self.areaBuses):\n",
        "            assert(len(state[j]) / 4 == len(buses))\n",
        "            state[j] = np.concatenate([state_bus[buses][:, i] for i in range(4)])\n",
        "        return state\n",
        "    \n",
        "    def get_state(self, ppc):\n",
        "        state = [np.zeros(dim) for dim in self.state_dim]\n",
        "        state_bus = ppc['bus'][:, (PD, QD, VM, VA)]\n",
        "        state_gen = ppc['gen'][:, (GEN_BUS, PG, QG)]\n",
        "        state_bus = self._cal_inject_power(state_bus, state_gen)\n",
        "        for j, buses in enumerate(self.areaBuses):\n",
        "            assert(len(state[j]) / 4 == len(buses))\n",
        "            state[j] = np.concatenate([state_bus[buses][:, i] for i in range(4)])\n",
        "        return state\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        '''\n",
        "        @action: List(np.array(batch_size, action_dim)), len = len(areaGens)\n",
        "        @state: List(np.array(batch_size, state_dim)), len = len(areaBuses)\n",
        "        '''\n",
        "        assert(len(action) == len(self.areaGens))\n",
        "        batch_size = action[0].shape[0]\n",
        "        state = [np.zeros((batch_size, dim)) for dim in self.state_dim]\n",
        "        reward = [np.zeros(batch_size) for _ in range(len(self.areaBuses))]\n",
        "        terminal = [np.zeros(batch_size) for _ in range(len(self.areaBuses))]\n",
        "        for i in range(batch_size):\n",
        "            # set voltage of generators\n",
        "            for j, gens in enumerate(self.areaGens):\n",
        "                assert(len(self.ppc['gen'][gens, VG]) == len(gens))\n",
        "                self.ppc['gen'][gens, VG] = action[j][i]\n",
        "            # calculate power flow\n",
        "            result, _ = runpf(self.ppc, self.ppopt)\n",
        "            # calculate the p, q, v, theta of buses\n",
        "            result_bus = result['bus'][:, (PD, QD, VM, VA)]\n",
        "            result_gen = result['gen'][:, (GEN_BUS, PG, QG)]\n",
        "            result_bus = self._cal_inject_power(result_bus, result_gen)\n",
        "            for j, buses in enumerate(self.areaBuses):\n",
        "                assert(len(state[j][i]) / 4 == len(buses))\n",
        "                state[j][i] = np.concatenate([result_bus[buses][:, i] for i in range(4)])\n",
        "            # caculate the rewards\n",
        "            reward_bus = self._cal_reward_bus(result['bus'][:, VM])\n",
        "            for j, r in enumerate(reward):\n",
        "                # the voltages of all buses in [0.95, 1.05]\n",
        "                if np.where((0.95 <= result['bus'][:, VM]) * (result['bus'][:, VM] <= 1.05))[0].tolist() == list(range(len(result['bus'][:, VM]))):\n",
        "                    r[i] = np.sum(reward_bus) / np.arange(result['bus'].shape[0])\n",
        "                # the voltages of all buses in [0.8, 1.2], and some of them not in [0.95, 1.05]\n",
        "                elif np.where((0.8 <= result['bus'][:, VM]) * (result['bus'][:, VM]  <= 1.2))[0].tolist() == list(range(len(result['bus'][:, VM]))):\n",
        "                    for k, buses in enumerate(self.areaBuses):\n",
        "                        if k != j:\n",
        "                            r[i] += self.beta * np.sum(reward_bus[buses])\n",
        "                        else:\n",
        "                            r[i] += np.sum(reward_bus[buses])\n",
        "                    r[i] *= self.alpha\n",
        "                # v < 0.8 or v > 1.2 \n",
        "                else:\n",
        "                    r[i] = -5\n",
        "            # judge whether voltage violation in each area happened\n",
        "            for j, term in enumerate(terminal):\n",
        "                buses = np.array(self.areaBuses[j])\n",
        "                if ((0.95 <= result['bus'][buses][:, VM]) * (result['bus'][buses][:, VM] <= 1.05)).all():\n",
        "                    term[i] = True\n",
        "                else:\n",
        "                    term[i] = False\n",
        "        return state, reward, terminal, {}\n",
        "    \n",
        "    def _cal_inject_power(self, result_bus, result_gen):\n",
        "        '''\n",
        "        Inject power of buses\n",
        "        @result_bus: (num_bus, 4), PD, QD, VM, VA of all buses\n",
        "        @result_gen: (num_bus, 3), GEN_BUS, PG, QG\n",
        "        @result_concat: (num_bus, ), concat 2nd dimension\n",
        "        '''\n",
        "        for bus_i, pg, qg in result_gen:\n",
        "            result_bus[int(bus_i) - 1][0] -= pg # PD\n",
        "            result_bus[int(bus_i) - 1][1] -= qg # QD\n",
        "        return result_bus\n",
        "\n",
        "    def _cal_reward_bus(self, result_v):\n",
        "        '''\n",
        "        @result_v: (num_bus, ) VM\n",
        "        '''\n",
        "        reward_bus = np.zeros(len(result_v))\n",
        "        idx, = np.where((0.95 <= result_v) * (result_v <= self.vref))\n",
        "        reward_bus[idx] = (result_v[idx] - 0.95) / (self.vref - 0.95)\n",
        "        idx, = np.where((self.vref < result_v) * (result_v  <= 1.05))\n",
        "        reward_bus[idx] = (1.05 - result_v[idx]) / (1.05 - self.vref)\n",
        "        idx, = np.where((0.8 <= result_v) * (result_v  < 0.95))\n",
        "        reward_bus[idx] = (self.vref - result_v[idx]) / (0.8 - self.vref)\n",
        "        idx, = np.where((1.05 < result_v) * (result_v  <= 1.2))\n",
        "        reward_bus[idx] = (result_v[idx] - self.vref) / (self.vref - 1.2)\n",
        "        idx, = np.where((result_v < 0.8) + (result_v > 1.2))\n",
        "        reward_bus[idx] = -5\n",
        "        return reward_bus\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import torch as t\n",
        "    import torch.nn as nn\n",
        "    from machin.frame.algorithms import MADDPG\n",
        "    from machin.utils.logging import default_logger as logger\n",
        "    from pypower.api import case39\n",
        "    # model definition\n",
        "    class Actor(nn.Module):\n",
        "        def __init__(self, state_dim, action_dim):\n",
        "            super(Actor, self).__init__()\n",
        "\n",
        "            self.fc1 = nn.Linear(state_dim, 16)\n",
        "            self.fc2 = nn.Linear(16, 16)\n",
        "            self.fc3 = nn.Linear(16, action_dim)\n",
        "\n",
        "        def forward(self, state):\n",
        "            a = t.relu(self.fc1(state))\n",
        "            a = t.relu(self.fc2(a))\n",
        "            a = t.tanh(self.fc3(a))\n",
        "            a = 1 + 0.2 * a\n",
        "            return a\n",
        "\n",
        "\n",
        "    class Critic(nn.Module):\n",
        "        def __init__(self, state_dim, action_dim):\n",
        "            # This critic implementation is shared by the prey(DDPG) and\n",
        "            # predators(MADDPG)\n",
        "            # Note: For MADDPG\n",
        "            #       state_dim is the dimension of all states from all agents.\n",
        "            #       action_dim is the dimension of all actions from all agents.\n",
        "            super(Critic, self).__init__()\n",
        "\n",
        "            self.fc1 = nn.Linear(state_dim + action_dim, 16)\n",
        "            self.fc2 = nn.Linear(16, 16)\n",
        "            self.fc3 = nn.Linear(16, 1)\n",
        "\n",
        "        def forward(self, state, action):\n",
        "            state_action = t.cat([state, action], 1)\n",
        "            q = t.relu(self.fc1(state_action))\n",
        "            q = t.relu(self.fc2(q))\n",
        "            q = self.fc3(q)\n",
        "            return q\n",
        "\n",
        "    agentBuses = [\n",
        "        [2, 14, 15, 16, 17, 18, 25, 26, 27, 28, 29, 30, 37, 38],\n",
        "        [13, 19, 20, 21, 22, 23, 24, 33, 34, 35, 36],\n",
        "        [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 31, 32, 39]\n",
        "    ]\n",
        "\n",
        "    agentBuses = [[b - 1 for b in buses] for buses in agentBuses]\n",
        "\n",
        "    agentGens = [\n",
        "        [0, 7, 8],\n",
        "        [3, 4, 5, 6],\n",
        "        [1, 2, 9]\n",
        "    ]\n",
        "\n",
        "    agent_num = len(agentGens)\n",
        "    states_dim = list(map(lambda x : 4 * x, list(map(len, agentBuses))))\n",
        "    actions_dim = list(map(len, agentGens))\n",
        "\n",
        "    maddpg = MADDPG(\n",
        "        [Actor(sdim, adim) for sdim, adim in zip(states_dim, actions_dim)],\n",
        "        [Actor(sdim, adim) for sdim, adim in zip(states_dim, actions_dim)],\n",
        "        [Critic(sum(states_dim), sum(actions_dim)) for _ in range(agent_num)],\n",
        "        [Critic(sum(states_dim), sum(actions_dim)) for _ in range(agent_num)],\n",
        "        [list(range(agent_num))] * agent_num,\n",
        "        t.optim.Adam,\n",
        "        nn.MSELoss(reduction='sum')\n",
        "    )\n",
        "    \n",
        "    ppc = case39()\n",
        "    powerenv = PowerNet(ppc, agentBuses, agentGens)\n",
        "    states = [t.tensor(st, dtype=t.float32).view(1, st.shape[0])\n",
        "                  for st in powerenv.reset()]\n",
        "    with t.no_grad():\n",
        "        result = maddpg.act([{\"state\": st} for st in states])\n",
        "        action = [act.numpy() for act in result]\n",
        "        states, rewards, violated, _ = powerenv.step(action)\n",
        "        states = [t.tensor(st, dtype=t.float32)\n",
        "                  for st in states]\n",
        "    maddpg.update()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m[2020-10-26 02:43:35,551] <WARNING>:default_logger:You have not specified the i/o device ofyour model <class 'torch.jit.RecursiveScriptModule'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by thisoperation.\u001b[0m\n",
            "\u001b[33m[2020-10-26 02:43:35,558] <WARNING>:default_logger:You have not specified the i/o device ofyour model <class 'torch.jit.RecursiveScriptModule'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by thisoperation.\u001b[0m\n",
            "\u001b[33m[2020-10-26 02:43:35,565] <WARNING>:default_logger:You have not specified the i/o device ofyour model <class 'torch.jit.RecursiveScriptModule'>, automatically determined and set to: cpu\n",
            "The framework is not responsible for any un-matching device issues caused by thisoperation.\u001b[0m\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}